
#+startup: content indent

[[file:../../index.org][Giovanni's Diary]] > [[file:../../subjects.org][Subjects]] > [[file:../programming.org][Programming]] > [[file:notes.org][Notes]] >

* Local LLMS
:PROPERTIES:
:RSS: true
:DATE: 10 Sep 2025 00:00:00 GMT
:CATEGORY: Programming
:AUTHOR: Giovanni Santini
:LINK: https://san7o.github.io/giovanni-diary/programming/notes/local-llms.html
:END:
#+INDEX: Giovanni's Diary!Programming!Notes!Local LLMS


** Ollama

[[https://github.com/ollama/ollama][Ollama]] is a simple frontend for [[https://github.com/ggml-org/llama.cpp][llama.cpp]] which is an inference
engine. You can download models from the ollama repository with:

#+begin_src bash
ollama pull ministral-3:3b
#+end_src

Additionally, all llama.cpp compatible models also work with
ollama. This means that you can download a =.gguf= model from a
different repository like [[https://huggingface.co/][huggingface]] and create a =Modelfile= like
this:

#+begin_src dockerfile
FROM ./LFM2.5-1.2B-Instruct-Q4_K_M.gguf
 
PARAMETER temperature 0.15
PARAMETER top_p 0.9
PARAMETER stop "</s>"
 
SYSTEM """
You are a helpful assistant.
"""
#+end_src

A more elaborated Modelfile would look like this:

#+begin_src dockerfile
FROM ./LFM2.5-1.2B-Instruct-Q8_0.gguf
 
PARAMETER temperature 0.15
PARAMETER top_p 0.9
PARAMETER stop "</s>"
  
TEMPLATE """{{- $lastUserIndex := -1 }}
{{- $hasSystemPrompt := false }}
{{- range $index, $_ := .Messages }}
{{- if eq .Role "user" }}{{ $lastUserIndex = $index }}{{ end }}
{{- if eq .Role "system" }}{{ $hasSystemPrompt = true }}{{ end }}
{{- end }}
{{- if not $hasSystemPrompt }}[SYSTEM_PROMPT]You are Ministral-3-3B-Instruct-2512, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.
You power an AI assistant called Le Chat.
Your knowledge base was last updated on 2023-10-01.
The current date is {{ currentDate }}.
 
When you are not sure about some information, or when the user's request requires up-to-date or specific data beyond your knowledge base, clearly state that you do not have the information and avoid making anything up.
 
If the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer, do not answer it right away. Instead, ask the user to clarify their request (e.g. "What are some good restaurants around me?" → "Where are you?" or "When is the next flight to Tokyo?" → "Where do you travel from?").
 
You are always very attentive to dates. You try to resolve relative dates (e.g. "yesterday" is {{ yesterdayDate }}), and when asked about information at specific dates, you discard information that applies to a different date.
 
You follow these instructions in all languages, and always respond to the user in the language they use or request.
 
# WEB BROWSING LIMITATIONS
 
You cannot perform any web search or access the internet, including opening URLs or links. If the user expects you to do so, clearly explain this limitation and ask them to paste the relevant content directly into the chat.
 
# MULTI-MODAL INSTRUCTIONS
 
You can read images, but you cannot generate images.
You cannot read, transcribe, or process audio files or videos.[/SYSTEM_PROMPT]
{{- end }}
{{- range $index, $_ := .Messages }}
{{- if eq .Role "system" }}[SYSTEM_PROMPT]{{ .Content }}[/SYSTEM_PROMPT]
{{- else if eq .Role "user" }}[INST]{{ .Content }}[/INST]
{{- else if eq .Role "assistant" }}
{{- if .Content }}{{ .Content }}
{{- if not (eq (len (slice $.Messages $index)) 1) }}</s>
{{- end }}
{{- end }}
{{- end }}
{{- end }}
"""
#+end_src

And load it with:

#+begin_src bash
ollama create mymodel:latest -f ./Modelfile
#+end_src

To read the modelfile of a model you downloaded from the ollama
repository, run:

#+begin_src bash
ollama show --modelfile ministral-3:3b
#+end_src


** Flowise

You can use [[https://github.com/FlowiseAI/Flowise][flowise]] to build AI agents.

-----

Travel: [[file:notes.org][Programming Notes]], [[../../theindex.org][Index]]
