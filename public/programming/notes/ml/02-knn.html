<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="/simple.css" />
<meta property="og:title" content="Giovanni's Diary">
<meta property="og:description" content="Diary of Giovanni's adventures.">
<meta property="og:image" content="https://giovanni-diary.netlify.app/logo.png">
<meta property="og:url" content="https://giovanni-diary.netlify.app/">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../../index.html">Giovanni's Diary</a> &gt; <a href="../../programming.html">Programming</a> &gt; <a href="../notes.html">Notes</a> &gt; <a href="intro-to-machine-learning.html">Intro to Machine Learning</a> &gt;
</p>

<div id="outline-container-orgad08b6c" class="outline-2">
<h2 id="orgad08b6c">The K-nearest neighbor algorithm</h2>
<div class="outline-text-2" id="text-orgad08b6c">
<p>
Given an n-dimensional partitioned space, and a new train value, we
want to associate the new value with a partition. A simple solution
could be to look at the nearest known data and their partition. To get
a more representative result, we can check k nearest neighbors.
</p>

<p>
More precisely, to classify an example \(d\):
</p>

<ul class="org-ul">
<li>find k nearest neighbors of \(d\)</li>
<li>choose as the label the majority label within the \(k\) nearest neighbors</li>
</ul>
</div>

<div id="outline-container-orge9ccb8c" class="outline-3">
<h3 id="orge9ccb8c">How do we measure "nearest"?</h3>
<div class="outline-text-3" id="text-orge9ccb8c">
<p>
Measuring distance / similarity is a domain-specific problem and there
are many different variations.
</p>

<ul class="org-ul">
<li><b><b>similarity</b></b>: numerical measure of how alike two data objects are</li>
<li><b><b>difference</b></b>: numerical measure of how different are two data objects</li>
</ul>
</div>

<div id="outline-container-org973a331" class="outline-4">
<h4 id="org973a331">Euclidean distance</h4>
<div class="outline-text-4" id="text-org973a331">
<p>
\[D(a, b) = \sqrt{(a_i - b_i)^2 + (a_2 - b_2)^2}\]
</p>

<p>
However, here we are assuming that all the labels are comparable
aka. measured in the same range. Therefore, data should be
<b><b>standardized</b></b>.
</p>
</div>
</div>

<div id="outline-container-org2630668" class="outline-4">
<h4 id="org2630668">Standardization or Z-score normalization</h4>
<div class="outline-text-4" id="text-org2630668">
<p>
Rescale the data so that the mean is 0 and the standard deviation from
the mean is 1
</p>

<p>
\[x_{norm}=\frac{x-\mu}{\sigma}\]
<b><b>Min-Max scaling</b></b>: scale the data to a fixed range - between 0 and 1
</p>

<p>
\[x_{norm}=\frac{x-x_{min}}{x_{max}-x_{min}}\]
</p>
</div>
</div>

<div id="outline-container-org140f4b6" class="outline-4">
<h4 id="org140f4b6">Minkowsky Distance</h4>
<div class="outline-text-4" id="text-org140f4b6">
<p>
Generalization of the Euclidean distance:
</p>

<p>
\[D(a, b) = (\sum_{k=1}^{p} |a_k - b_k|^r)^{\frac{1}{r}}\]
</p>
</div>
</div>

<div id="outline-container-org6cd772e" class="outline-4">
<h4 id="org6cd772e">Cosine Similarity</h4>
<div class="outline-text-4" id="text-org6cd772e">
<p>
Given two different vectors:
</p>

<p>
\[COS(d_1, d_2) = (d_1 * d_2) / ||d_1|| ||d_2||\]
</p>
<ul class="org-ul">
<li>\(*\) is the dot product and \(||d||\) is the length of vector d</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org4278648" class="outline-3">
<h3 id="org4278648">Decision Boundaries</h3>
<div class="outline-text-3" id="text-org4278648">
<p>
The decision boundaries are places in the features space where the
classification of a point / example changes.
</p>
</div>

<div id="outline-container-orgd5518cf" class="outline-4">
<h4 id="orgd5518cf">Voronoi Diagram / Partitioning</h4>
<div class="outline-text-4" id="text-orgd5518cf">
<p>
Describes the areas that are nearest to any given point, given a set
of data
</p>

<ul class="org-ul">
<li>each line segment is equidistant between two points</li>
</ul>

<p>
However, k-NN does not explicitly compute decision boundaries, but
form a subset of the Voronoi diagram for the training data.
</p>
</div>
</div>
</div>

<div id="outline-container-org9bd9ea9" class="outline-3">
<h3 id="org9bd9ea9">How to pick K</h3>
<div class="outline-text-3" id="text-org9bd9ea9">
<p>
Some techniques to pick \(k\) include:
</p>

<ul class="org-ul">
<li>common heuristics</li>
<li>use validation set</li>
<li>use cross validation</li>
<li>rule of thumb is \(k < \sqrt{n}\) where n is the size of training
examples</li>
</ul>

<p>
k-NN variations: weighted k-NN.
</p>
</div>
</div>

<div id="outline-container-org0d30fb3" class="outline-3">
<h3 id="org0d30fb3">Lazy Learner vs Eager Learner</h3>
<div class="outline-text-3" id="text-org0d30fb3">
<p>
k-NN belongs to the class of lazy learning algorithms
</p>

<ul class="org-ul">
<li><b><b>lazy learning</b></b>: simply stores training data and operates when it
is given a test example</li>
<li><b><b>eager learning</b></b>: given a training set, constructs a
classification model before receiving new test data to classify</li>
</ul>

<p>
This means that k-NN is not really fast during inference, but no training is required.
</p>
</div>
</div>

<div id="outline-container-org877f88d" class="outline-3">
<h3 id="org877f88d">Curse of Dimensionality</h3>
<div class="outline-text-3" id="text-org877f88d">
<p>
In high dimensions almost all points are far away from each other
</p>

<ul class="org-ul">
<li>the size of the data space grows exponentially with the number of dimensions</li>
<li>this means that the size of the data set must also grow
exponentially in order to keep the same density
<ul class="org-ul">
<li>the success of k-NN is very dependent on having a dense data set</li>
<li>k-NN requires a point to be close in every single dimension</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org14c89fb" class="outline-3">
<h3 id="org14c89fb">Computational Cost</h3>
<div class="outline-text-3" id="text-org14c89fb">
<ul class="org-ul">
<li>Linear algorithm (no preprocessing) is O(kN) to compute the distance
for all N datapoints</li>
<li>tree-based data structures: pre-processing often using K-D tree
<ul class="org-ul">
<li>divide the space in regions. To check which region a point belongs
to, simply traverse a binary tree.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org334536d" class="outline-3">
<h3 id="org334536d">Parametric vs Non-parametric Models</h3>
<div class="outline-text-3" id="text-org334536d">
<ul class="org-ul">
<li><b><b>Parametric models</b></b> have a finite number of parameters
<ul class="org-ul">
<li>linear regression, logistic regression, and linear support vector
machines</li>
</ul></li>
<li><b><b>Nonparametric model</b></b>: the number of parameters is (potentially)
infinite
<ul class="org-ul">
<li>k-nearest neighbor, decision trees, RBF kernel SVMs</li>
</ul></li>
</ul>

<hr />

<p>
Travel: <a href="intro-to-machine-learning.html">Intro to Machine Learning</a>, <a href="../../../theindex.html">Index</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
