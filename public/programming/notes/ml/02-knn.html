<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="/simple.css" />
<link rel="stylesheet" href="/giovanni-diary/simple.css" />
<meta property="og:title" content="Giovanni's Diary">
<meta property="og:description" content="Diary of Giovanni's adventures.">
<meta property="og:image" content="https://giovanni-diary.netlify.app/logo.png">
<meta property="og:url" content="https://giovanni-diary.netlify.app/">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../../index.html">Giovanni's Diary</a> &gt; <a href="../../../subjects.html">Subjects</a> &gt; <a href="../../programming.html">Programming</a> &gt; <a href="../notes.html">Notes</a> &gt; <a href="intro-to-machine-learning.html">Intro to Machine Learning</a> &gt;
</p>

<div id="outline-container-org5891ffc" class="outline-2">
<h2 id="org5891ffc">The K-nearest neighbor algorithm</h2>
<div class="outline-text-2" id="text-org5891ffc">
<p>
In this chapter we will discuss our first supervised learning
algorithm: <span class="underline">K-nearest neighbors</span>, aka <span class="underline">K-NN</span>. Remember that supervised
learning means that the algorithm is given data in an $n$-dimensional
space with labels, we will now focus on the classification problem
where the algorithm should predict the label of new unseen data.
</p>

<p>
If we make the only assumption that data with similar labels are
somehow "cose" together in the space, a simple solution to this
problem could be to look at the nearest known data and their label. We
will discuss the concept distance later. Moreover, we can average over
\(k\) nearest neighbors to get a more representative result.
</p>

<p>
More precisely, to <span class="underline">classify</span> an example \(d\):
</p>

<ul class="org-ul">
<li>find \(k\) nearest neighbors of \(d\)</li>
<li>choose as the label the majority label within the \(k\) nearest neighbors</li>
</ul>

<p>
An example speudocode:
</p>

<pre class="example">
KNN(input, K, DATA) -&gt; {1, ..., m}:
  int label_count[m] = {0}
  bool used[#DATA] = {false}
  for k=0 to K do:
    int max_distance = 0
    int max_input = 0
    for x_i, y_i in DATA do:
      d = distance(x_i, input)
      if d &gt; max_distance and not used[x_i] then:
        max_distance = d
        max_input = x_i
    label_count[DATA[x_i]] += 1
    used[x_i] = true
  return max_index(label_count)
</pre>
</div>

<div id="outline-container-org1df9b5e" class="outline-3">
<h3 id="org1df9b5e">How do we measure distance?</h3>
<div class="outline-text-3" id="text-org1df9b5e">
<p>
Measuring distance / similarity is a domain-specific problem and there
are many different alternatives. We will now see different methods to
model distance.
</p>
</div>

<div id="outline-container-orgd10ba9d" class="outline-4">
<h4 id="orgd10ba9d">Euclidean distance</h4>
<div class="outline-text-4" id="text-orgd10ba9d">
<p>
Euclidean distance is measured in \(n\) dimensions with the formula:
</p>

<p>
\[D(a, b) = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2+...}\]
</p>

<p>
Note that here we are assuming that all the labels are comparable,
meaning measured in the same range. A trivial example can be made to
motivate this: a certain dimension may represent the cost of an item
in euros and some other dimension may measure it in dollars, to find
the difference we need to first convert everything to the same
currency. Therefore data should be <span class="underline">standardized</span>, we will now see two
common transformations:
</p>

<ul class="org-ul">
<li><p>
<span class="underline">Standardization or Z-score normalization</span>: Rescale the data so that
the mean is 0 and the standard deviation from the mean is 1.
</p>

<p>
\[x_{norm}=\frac{x-\mu}{\sigma}\]
</p></li>

<li><p>
<span class="underline">Min-Max scaling</span>: scale the data to a fixed range - between 0 and 1.
</p>

<p>
\[x_{norm}=\frac{x-x_{min}}{x_{max}-x_{min}}\]
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org3068a0a" class="outline-4">
<h4 id="org3068a0a">Manhatthan Distance</h4>
<div class="outline-text-4" id="text-org3068a0a">
<p>
The Manhatthan distance is used to find the distance of two paths in a
grid, it was originally invented for the city o Manatthan
</p>

<p>
\[D(a, b)=\sum_i |a_i - b_i|\]
</p>
</div>
</div>

<div id="outline-container-orgfa9055a" class="outline-4">
<h4 id="orgfa9055a">Checysher Distance</h4>
<div class="outline-text-4" id="text-orgfa9055a">
<p>
The checkysher distance is useful to calculate distances in the chess
board, in particular the number of moved of the kind to reach a
certain square.
</p>

<p>
\[D(a, b)=max_i|a_i-b_i|\]
</p>
</div>
</div>

<div id="outline-container-org58ed429" class="outline-4">
<h4 id="org58ed429">Minkowsky distance</h4>
<div class="outline-text-4" id="text-org58ed429">
<p>
All the three aforementioned distances can be generalized with the
following formula (Minkowsky Distance) also called $p$-norm or
\(L_p\):
</p>

<p>
\[D(a, b) = (\sum_{k=1}^{n} |a_k - b_k|^p)^{\frac{1}{p}}\]
</p>

<ul class="org-ul">
<li>\(p=1\): Manhattan distance</li>
<li>\(p=2\): Euclidean distance</li>
<li>\(p\to\infty\): Checkysher distance</li>
</ul>

<p>
\(L_1\) is popular because it tends to result is sparse
solutions. However, it is not differentiable, so It only works for
gradient descent solvers. \(L_2\) is also popular because for some loss
functions, it can be solved directly. \(L_p\) is less popular since they
don't tend to shrink the weights enough.
</p>
</div>
</div>

<div id="outline-container-orgfd952d4" class="outline-4">
<h4 id="orgfd952d4">Cosine Similarity</h4>
<div class="outline-text-4" id="text-orgfd952d4">
<p>
Given two different vectors \(d_1\) and \(d_2\), we can find the cosine
\(cos\) with the formula:
</p>

<p>
\[cos(d_1, d_2) = (d_1 \cdot d_2) / ||d_1|| ||d_2||\]
</p>

<ul class="org-ul">
<li>where \(\cdot\) is the dot product and \(||d||\) is the length of vector \(d\)</li>
</ul>

<p>
In fact, this formula is derived from the definition of dot product: \(d_1 \cdot d_2 = ||d_1||||d_2||cos(\theta)\)
</p>

<p>
Cosine similarity does not depend on the magnitudes of the vectors,
but only on their angle. For example, two proportional vectors have a
cosine similarity of \(+1\), two orthogonal vectors have a similarity of
\(0\), and two opposite vectors have a similarity of \(-1\).
</p>

<p>
For example, in information retrieval and text mining, each word is
assigned a different coordinate and a document is represented by the
vector of the numbers of occurrences of each word in the
document. Cosine similarity then gives a useful measure of how similar
two documents are likely to be, in terms of their subject matter, and
independently of the length of the documents.
</p>
</div>
</div>
</div>

<div id="outline-container-org13c4e90" class="outline-3">
<h3 id="org13c4e90">Decision Boundaries</h3>
<div class="outline-text-3" id="text-org13c4e90">
<p>
The KNN algorithm can be thought of as assigning a label to an object
withing the space enclosed by a <span class="underline">decision boundary</span>. Decision
boundaries are places in the features space where the classification
of a point / example changes.
</p>
</div>

<div id="outline-container-org588e334" class="outline-4">
<h4 id="org588e334">Voronoi Diagram / Partitioning</h4>
<div class="outline-text-4" id="text-org588e334">
<p>
A Voronoi diagram describes the areas that are nearest to any given
point, given a set of data where each line segment is equidistant
between two points. More formally, the Voronoi region \(R_k\) associated
with a subset of the points in the space \(P_k\) is defined as:
</p>

<p>
\[R_k = \{ x\in X\ |\ d(x, P_K)\le d(x, P_j)\ for\ all\ j\ne k  \}\]
</p>

<p>
KNN does not explicitly compute decision boundaries, but form a subset
of the Voronoi diagram for the training data.
</p>
</div>
</div>
</div>

<div id="outline-container-org7aafcb8" class="outline-3">
<h3 id="org7aafcb8">Choosing K</h3>
<div class="outline-text-3" id="text-org7aafcb8">
<p>
Some techniques to pick \(k\) include:
</p>

<ul class="org-ul">
<li>common heuristics</li>
<li>use validation set</li>
<li>use cross validatoin</li>
<li>rule of thumb is k &lt; sqrt(n) where n is the size of training examples</li>
</ul>

<p>
In general, bugger values of \(k\) give a smoother decision boundary.
</p>
</div>
</div>

<div id="outline-container-org2a10300" class="outline-3">
<h3 id="org2a10300">Lazy Learner vs Eager Learner</h3>
<div class="outline-text-3" id="text-org2a10300">
<p>
k-NN belongs to the class of lazy learning algorithms.
</p>

<ul class="org-ul">
<li><span class="underline">lazy learning</span>: simply stores training data and operates when it is
given a test example. Note that if the data is large, the machine
may run out of memory.</li>
<li><span class="underline">eager learning</span>: given a training set, constructs a classification
model before receiving new test data to classify</li>
</ul>

<p>
This means that k-NN is not really fast during inference, but no training is required.
</p>
</div>
</div>

<div id="outline-container-org10bd07f" class="outline-3">
<h3 id="org10bd07f">Curse of Dimensionality</h3>
<div class="outline-text-3" id="text-org10bd07f">
<p>
There is a big problem in high dimensional data that may degrade the
performance of the algorithm. That is, in high dimensions almost all
points are far away from each other. I will now proceed to motivate
this.
</p>

<p>
Suppose you have a space in \(n\) dimensions where \(m\) points are
distributed uniformly. The volume of the space would be \(S^n\) where
\(S\) is a measure of a side or the size of a domain in space, assuming
all have the same domain. You could quantify a certain data density
quantity as \(\delta = \frac{m}{S^n}\). If we increase the dimensions by
one, the density would decrease by a factor of \(S\), hence we need to
have \(S\) times the original data size \(m\) to get the same density:
\(\delta' = \frac{mS}{S^{n+1}}\). In general,
\(\delta''=\frac{mS^k}{S^{n+k}}\), for this reason we say that the size
of the input has to grow exponencially with the dimensions. A less
dense space means that all the data is more spread apart, hence all
points are far away from each other.
</p>

<p>
The success of KNN is very dependent on having a dense data set since
it requires points to be close in every dimension.
</p>
</div>
</div>

<div id="outline-container-org6e3aaee" class="outline-3">
<h3 id="org6e3aaee">Computational Cost</h3>
<div class="outline-text-3" id="text-org6e3aaee">
<ul class="org-ul">
<li>Linear algorithm (no preprocessing) is \(O(kN)\) to compute the
distance for all N datapoints</li>
<li>\(O(klog(n))\) for tree-based data structures: pre-processing often
using K-D tree
<ul class="org-ul">
<li>divide the space in regions. To check which region a point belongs
to, simply traverse a binary tree.</li>
</ul></li>
</ul>

<p>
k-NN variations: weighted k-NN where closest neighbors contribute the most.
</p>

<hr />

<p>
Travel: <a href="intro-to-machine-learning.html">Intro to Machine Learning</a>, <a href="../../../theindex.html">Index</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
