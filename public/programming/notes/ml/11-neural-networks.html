<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />

<link rel="stylesheet" href="https://san7o.github.io/micro-style.css/micro-style.css" />
<style>
  html, body {
      height: 100%;
      margin: 0;
  }

  body {
      display: grid;
      place-items: center;  /* centers vertically & horizontally */
  }
</style>
<link rel="icon" href="/giovanni-diary/favicon.ico" type="image/x-icon">
<meta property="og:title" content="Giovanni's Diary">
<meta property="og:description" content="Diary of Giovanni's adventures.">
<meta property="og:image" content="https://san7o.github.io/giovanni-diary/logo.png">
<meta property="og:url" content="https://san7o.github.io/giovanni-diary/">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../../index.html">Giovanni's Diary</a> &gt; <a href="../../../subjects.html">Subjects</a> &gt; <a href="../../programming.html">Programming</a> &gt; <a href="../notes.html">Notes</a> &gt; <a href="intro-to-machine-learning.html">Intro to Machine Learning</a> &gt;
</p>
<div id="outline-container-org7620b40" class="outline-2">
<h2 id="org7620b40">Neural Networks</h2>
<div class="outline-text-2" id="text-org7620b40">
<p>
Previously we have discussed perceptrons as a non-linear parameterized
function with restricted output range. It predicts the output by
learning the weights while considering a bias, more specifically it
associates one weight per input and multiplies them together.
</p>

<p>
\[a=h(\sum_i w_ix_i+b)\]
</p>

<p>
Perceptrons are great and easy to understand, however they assume the
input data is linearly separable. Indeed, in 1969 Minsky and Papert
showed that an exclusive OR (XOR) cannot be solved with a
perceptron. This limitation seemd to halt the development of the
field, and commenced what is now known as the "AI Winter". Only in
1986 progress was made with a solution that would change the world:
<span class="underline">multi-layer perceptrons</span>.
</p>

<p>
In essence, multi-layer perceptrons are a set of densly connected
perceptrons (aka neurons) organized in layers. Input goes from the
first layer to the next until the last one where the output is
collected, if each neuron is connected to all the neurons in the
previous layer then we refer to the network as a <span class="underline">fully connected
network</span>. This design was inspired by neuroscience by how the brain
manages to understand complex input through billions and billions of
connections between neurons.
</p>

<p>
Let's now further formalize a multi-layer perceptron, aka a <span class="underline">neural
network</span>. Each neuron belongs to a layer \(l\in \{ 0, ..., L \}\) where
\(0\) is the <span class="underline">input</span> layer and \(L\) is the <span class="underline">output</span> layer. All the layers
in between are called <span class="underline">hidden layers</span>. The symbol \(z^{(l)}_i\)
indicates the output of the $i$-th neuron in the layer \(l\). A single
neuron on layer \(l\) takes the results \(z_{i}^{(l-1)}\) from the \(i\)
nodes of the \((l-1)\) layer and sums them, multiplying them by the
weights \(w_{ij}^{(l)}\), and adds a bias \(b_l\). We call this value
\(a_i^{(l)}\):
</p>

<p>
\[a_{i}^{(l)}(x, w)=\sum_j w_{ij}z_j^{(l-1)}(x, w)+b_l\]
</p>

<p>
The node then passes this result to a non-linear activation function \(h(x)\), we call this number \(z_{i}^{(l)}\):
</p>

<p>
\[z_i^{(l)}(x, w)=h(a_i^{(l)}(x, w))\]
</p>

<p>
There are many non-linear activation function, a popular one is called
<span class="underline">Rectified Linear Units</span> aka ReLU and it looks like this:
</p>

<p>
\[ReLU(x)=max(0, x)\]
</p>

<p>
The output layer is usually multiplied by a \(\sigma\) function that
transforms the outputs to a probability or smooth the value, such as
the <span class="underline">sigmoid</span> function:
</p>

<p>
\[\sigma(x)= \frac{1}{1+exp(-x)}\]
or the <span class="underline">softmax</span> function:
</p>

<p>
\[\sigma (z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}\]
</p>

<p>
For example, a multi layer perceptron with one input, one hidden and
one output layer takes the form:
</p>

<p>
\[y_k(x, w)=\sigma (\sum_{j=2}^M w_{kj}^{(2)}h(\sum_{i=1}^D w_{ji}^{(1)}x_i + b_1) + b_{2}\]
</p>

<p>
The process of evaluating the previous formula can be interpreted as
<span class="underline">forward propagation</span> of information through the network, for this
reason we also call this types of networks a <span class="underline">feed forward</span> network.
</p>

<p>
We know how to find the output from the inputs, we will now discuss
how to train the network. We cannot train the multi player perceptron
like we have seen for a single perceptron because we do not know the
desired target output for the hidden layers. We will see how this is
solved through a technique called <span class="underline">backpropagation</span>.
</p>
</div>
<div id="outline-container-org93c3cd7" class="outline-3">
<h3 id="org93c3cd7">Training with backpropagation</h3>
<div class="outline-text-3" id="text-org93c3cd7">
<p>
Given training samples \(T=\{ (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)
\}\), we want to adjust all the weights of the network \(\Theta\) such
that an error (also called loss) function \(E(\cdot)\) is minimized.
</p>

<p>
\[min_{\Theta} \sum _i E^{(L)}(y_i, f(x_i;\Theta))\]
</p>

<p>
Where \(y_i\) is the expected output of the $i$-th neuron. There are
many choices for an error function, such as square loss:
</p>

<p>
\[E(x, w)^{(L)}=\frac{1}{2}\sum_i (y_i-z_i^{(L)}(x, w))^2\]
</p>

<p>
or cross entropy:
</p>

<p>
\[E(x, w)^{(L)} = -\sum_i y_i \log(z_i^{(L)}(x, w))\]
</p>

<p>
For training will again use gradient descent, however we need a way to
propagate the error in order to compute the gradient of the hidden
layers. To do so we use an algorithm called backpropagation, which is
composed of the following steps:
</p>

<ol class="org-ol">
<li><span class="underline">Feedforward propagation</span>: Accept input \(x_i\), then pass through
the intermediate stages and obtain the output.</li>
<li>Use the computed output to compute a <span class="underline">scalar cost</span> depending on the
loss function.</li>
</ol>

<p>
In order to update the weights based on the error, we want to find the
partial derivative of the error with respect to a weight \(w_{ij}\), and
we write \(\frac{\partial E(x, w)^{(l)}}{\partial w_{ij}}\). We can
apply the chain rule \(D(f(g(x)))=f'(g(x))g'(x)\) which can be rewritten
as \(\frac{\partial f}{\partial g}=\frac{\partial f}{\partial
g}\frac{\partial g}{\partial x}\) to the error function:
</p>

<p>
\[\frac{\partial E^{(L)}}{\partial w_{ij}}=\frac{\partial E^{(l)}}{\partial z_i^{(l)}}\frac{\partial z_i^{(l)}}{\partial a_i^{(l)}}\frac{\partial a_i^{(l)}}{\partial w_{ij}}\]
</p>

<p>
Let's analyze all three of the factors:
</p>

<p>
\[\frac{\partial E^{(L)}}{\partial z_i^{(l)}}=\frac{\partial}{\partial z_i^{(l)}}\frac{1}{2}\sum_i (y_i-z_i^{(l)}(x, w))^2=-(y_i-z_i^{(l)})\]
\[\frac{\partial z_i^{(l)}}{\partial a_i^{(l)}}=\frac{\partial h}{\partial a_i^{(l)}}\]
\[\frac{\partial a_i^{(l)}}{\partial w_{ij}}=\frac{\partial}{\partial w_{ij}}\sum_j w_{ij}z_j^{(l-1)}(x, w)=z_j^{(l-1)}\]
</p>

<p>
So overall, for the first layer you can use:
</p>

<p>
\[\frac{\partial E^{(L)}}{\partial w_{ij}}=-\frac{\partial h}{\partial a_i^{(l)}}(y_i-z_i^{(l)})z_j^{(l-1)}\]
</p>

<p>
Where the derivate of the activation function depends on the function. \((y_i-z_i^{(l)})\) is often referred to as \(\delta_i^{(l)}\).
</p>

<p>
For the hidden layers you do not know the expected output \(y_i\) directly, so you use the formula:
</p>

<p>
\[\frac{\partial E^{(l)}}{\partial w_{ij}}=-\frac{\partial h}{\partial a_i^{(l)}}(\sum_j\delta^{(l+1)}_iw_{ij}^{(l+1)}) z_j^{(l-1)}\]
</p>

<p>
We can finally perform gradient descent like we discussed in its own chapter.
</p>
</div>
</div>
<div id="outline-container-org5b1138b" class="outline-3">
<h3 id="org5b1138b">Example</h3>
<div class="outline-text-3" id="text-org5b1138b">
<p>
We will relax the notation now. Consider a two-layer network (that does not count the input layer). Given:
</p>

<p>
\[h(a)\equiv tanh(a)\]
\[tanh(a)=\frac{e^a-e^{-a}}{e^a+e^{-a}}\]
\[h'(a)=1-h(a)^2\]
\[E^{(L)} = \frac{1}{2}\sum_{i} (y_i - z_i^{(L)})^2\]
</p>

<p>
where \(z_i^{(L)}\) is the activation of output unit \(i\), and \(y_i\) is the corresponding target, for a particular input pattern \(x_n\).
</p>

<p>
For each pattern in the training set in turn, we first perform forward propagation using:
</p>

<p>
\[a_i^{(0)} = \sum_{i=1}^D w_{li}^{(0)}x_i\]
\[z_i^{(0)} = tanh(a_i^{(0)})\]
\[a_j^{(1)} = \sum_{j=1}^M w_{ij}z_i^{(0)}\]
</p>

<p>
Next we compute the \(\delta\)'s for each output unit using:
</p>

<p>
\[\delta _i^{(1)} = y_i - z_i^{(1)}\]
</p>

<p>
Then we backpropagate there to obtain $&delta;$s for the hidden units using:
</p>

<p>
\[\delta_j^{(0)} = (1-(z_j^{(1)})^2)(\sum_{k=1}^K w_{kj}\delta_k^{(1)})z_k^{(0)}\]
</p>

<p>
Finally, the derivatives with respect to the first-layer and second-layer weights are given by:
</p>

<p>
\[\frac{\partial E_i^{(1)}}{\partial w_{ji}}=\delta_i^{(1)}z_j^{(0)},\ \frac{\partial E_j^{(0)}}{\partial w_{kj}}=\delta _k^{(0)}x_j\]
</p>

<p>
To summarize, with a simpler notation:
</p>

<ol class="org-ol">
<li>Apply an input vector \(x_n\) to the network and forward propagate
through the network using \(a_j=\sum_iw_{ji}z_i\) and \(z_j=h(a_j)\) to
find the activations of all the hidden and output units</li>
<li>Evaluate the \(\delta_k\) for all the output units using \(\delta_k=y_k-z_k\)</li>
<li>Backpropagate the \(\delta\)'s using \(\delta_j=h'(a_j)\sum_k
   w_{kj}\delta_k\) to obtain \(\delta_j\) for each hidden unit in the
network</li>
<li>Use \(\frac{\partial E_n}{\partial w_{ji}}=\delta_jz_i\) to evaluate
the required derivatives</li>
</ol>
</div>
</div>
<div id="outline-container-org38bd3c5" class="outline-3">
<h3 id="org38bd3c5">Convolutional Neural Networks</h3>
<div class="outline-text-3" id="text-org38bd3c5">
<p>
In computer vision, we have a lot of important spacial information we
want to model, however this happens to be really difficult for neural
network as were described previously. The problem raises from the fact
that it is difficult to pass an image as input to a neural
network. One possibility would be to set an input node for each pixel,
however this has many problems primarily because the input layer would
be huge (a 500 by 500 image will have 250000 inputs) and all the
spacial information would be lost, like being able to encode what is
nearby a pixel. For these reasons, a special solution was needed.
</p>

<p>
By the same time backpropagation was discovered, <span class="underline">convolutional neural
networks</span> were proposed as a solution to the problem inspired by the
mammalian visual cortex. This type of neural networks use convolution
in place of general matrix multiplication in at least one of their
layers. A convolution is a general purpose filter operation for image
where a kernel matrix is applied to an image and the central pixel is
determined by adding the weighted values of all its neighbors
together, producing a <span class="underline">feature map</span>.
</p>

<p>
\[S(i, j) = (I*K)(i, j) = \sum_{m=-i}^i\sum_{n=-j}^jI(m, n)K(i-m,j-n)\]
</p>

<p>
These convolutions matrices are learned by the convolutional neural
network, this is how it can understand spacial features such as edges
or certain arrangements of pixels on its own. Suppose we are training
a CNN to classify human faces, the network may learn the features of
the eyes and their position with respect to the nose and the mouth, it
may catch the shape of the ears or the color of the skin, and so on.
</p>

<p>
Convolutional neural networks are feedforward neural networks composed
of a set of filters that cover a spacially small portion of the input
data and that are convoluted over it. The network may have many
convolutions over the same input, and convolutions over
convolutions. For classification, it is common to have multiple
convolutions chained together while reducing the spacial size of the
representation and increasing its depth with a process known as
<span class="underline">spacial polling</span>, until the size is \(1\). Then, all the data is fed to
a traditional neural network for classification.
</p>
</div>
</div>
<div id="outline-container-orga898285" class="outline-3">
<h3 id="orga898285">Other neural networks</h3>
<div class="outline-text-3" id="text-orga898285">
<p>
Many other types of neural networks were developed over time for
domain specific problems, some of the notable ones are:
</p>

<ul class="org-ul">
<li><span class="underline">Recurrent network</span>: nodes on the same layer influence each other,
used in video frame prediction.</li>
<li><span class="underline">Autoencoders</span>: unsupervised approach for learning a
lower-dimensional feature representation from unlabeled training
data. Additionally, we can train a decoder to do the opposite. We
will talk more about this in the following chapter.</li>
</ul>

<hr />

<p>
Travel: <a href="intro-to-machine-learning.html">Intro to Machine Learning</a>, <a href="../../../theindex.html">Index</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
