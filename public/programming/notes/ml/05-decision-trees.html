<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="/simple.css" />
<meta property="og:title" content="Giovanni's Diary">
<meta property="og:description" content="Diary of Giovanni's adventures.">
<meta property="og:image" content="https://giovanni-diary.netlify.app/logo.png">
<meta property="og:url" content="https://giovanni-diary.netlify.app/">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../../index.html">Giovanni's Diary</a> &gt; <a href="../../../subjects.html">Subjects</a> &gt; <a href="../../programming.html">Programming</a> &gt; <a href="../notes.html">Notes</a> &gt; <a href="intro-to-machine-learning.html">Intro to Machine Learning</a> &gt;
</p>

<div id="outline-container-org08a78b8" class="outline-2">
<h2 id="org08a78b8">Decision Trees</h2>
<div class="outline-text-2" id="text-org08a78b8">
<p>
Decision trees are simple, intuitive and powerful supervised learning
algorithms used for both classification and regression. They make
predictions by recursively splitting on different attributes according
to a <span class="underline">tree structure</span>. For continuous attributes splitting is based on
less than or greater than some threshold. During testing time, the
program will walk the tree from the root, based on the split condition
at each intersection, until It reaches the leaf nodes.
</p>

<p>
Given a training dataset, we need to learn:
</p>

<ul class="org-ul">
<li>which attribute to choose for splitting</li>
<li>what value of that attribute to use for splitting</li>
</ul>
</div>

<div id="outline-container-orgab4f274" class="outline-3">
<h3 id="orgab4f274">Classification and Regression</h3>
<div class="outline-text-3" id="text-orgab4f274">
<p>
Training examples that fall into the region \(R_m\)
\[\{ (x^{m_1}, t^{m_1}),...,(x^{m_k}, t^{m_k}) \}\]
</p>

<p>
<span class="underline">Classification</span> tree:
</p>

<ul class="org-ul">
<li>discrete output \(y \in \{1, ..., C\}\)</li>
<li>after traversing the tree, the final leaf value \(y^m\) is typically
set to the most common value in \(\{t^{m_1}, ..., t^{m_k}\}\) which is
the set of labels in the region, i.e.</li>
</ul>
<p>
\[y^m = argmax_{t\in \{ 1, ..., C \}} \sum_{m_i}\mathbb{1}_{t} (t^{m_i}) \]
	Where \(\mathbb{1} (x)\) is the indicator function defined as:
\[\mathbb{1}_A: X\rightarrow \{ 0, 1 \}\]
\[\mathbb{1}_A(x) \equiv
\begin{cases}
1,\ x\in A \\
0,\ x \notin A
\end{cases}\]
<span class="underline">Regression</span> tree:
</p>

<ul class="org-ul">
<li>continuous output: \(y\in \mathbb{R}\)</li>
<li>the leaf value \(y^m\) typical set to the mean value in \(\{t^{m_1},...,t^{m_k}\}\)</li>
</ul>
</div>
</div>

<div id="outline-container-org758ca58" class="outline-3">
<h3 id="org758ca58">Train a decision tree</h3>
<div class="outline-text-3" id="text-org758ca58">
<p>
We want to find a simple tree that explains data well. A good enough
approach would be to use a greedy heuristic: start with an empty
decision tree and complete the training set by recursively "splitting"
the best attributes.
</p>

<p>
We will now discuss three ways to decide which split is the best.
</p>
</div>

<div id="outline-container-org4d52203" class="outline-4">
<h4 id="org4d52203">Accuracy Gain</h4>
<div class="outline-text-4" id="text-org4d52203">
<p>
A <span class="underline">gain</span> is defined based on change in some criteria before and after
a split. Let us define <span class="underline">accuracy gain</span>.
</p>

<ul class="org-ul">
<li>suppose we have a region \(R\). Denote the misclassification error of that region as $L(R)  $ which is the fraction of incorrect classification</li>
<li>we split \(R\) into two regions \(R_1\) and \(R_2\) based on some attribute</li>
<li>the error after the split is:
\[\frac{|R_1|}{|R|}L(R_1) + \frac{|R_2|}{|R|}L(R_2)\]
where \(|R|\) is the number of samples in \(R\).</li>
<li>we define the accuracy gain as:
\[AG = L(R) - \frac{|R_1| L(R_1) + |R_2|L(R_2)}{|R|}\]</li>
</ul>

<p>
A big value of \(AG\) means that the split is good. If the error \(L(R)\)
after the split is as good as before, \(AG\) is \(0\), and if the split
introduced more error then \(AG\) is negative.
</p>

<p>
We defined a function to measure how good a split is, let's look at
another possibility inspired by information theory.
</p>
</div>
</div>

<div id="outline-container-org457eb17" class="outline-4">
<h4 id="org457eb17">Basic of information theory and entropy</h4>
<div class="outline-text-4" id="text-org457eb17">
<p>
Which coin result is more uncertain between those two?
</p>

<ul class="org-ul">
<li>0001000000000000010001</li>
<li>0101100101010011101010101</li>
</ul>

<p>
Entropy is a measure of expected "surprise". How uncertain are we of the value of a draw from this distribution?
\[H(X) = -\mathbb{E}_{X\sim p}[\log_2 p(X)] = -\sum_{x\in X} p(x)\log_2p(x)\]
</p>

<ul class="org-ul">
<li>the greater the entropy, the less certain we are. The more certain we are, the less entropy
<ul class="org-ul">
<li>remember that entropy is the measure of disorder</li>
</ul></li>
</ul>

<p>
Meaning: we are taking the probability of the "surprise" of an event
\(E\) with the formula \(\log(\frac{1}{p(E)})\) which can be rewritten as
\(-\log(p(E))\). This formula means that the more likely the event, the
closes it's surprise will be 0. The opposite is true: the less
probable the event, the greater the value of the log function. Since
we are taking the expected value, the more unexpected is the
probability of the event, the higher the entropy. The formula with the
sum is derived by directly applying the definition of expected value
\(\mathbb{E}[x]=\sum_{i=1}x_i p_i\)
</p>

<p>
On high entropy, the variable has a more uniform-like distribution,
meaning that It is more difficult to predict an event (so It surprises
us). On low entropy, the distribution has peaks and valleys were some
events are frequent and some are not (so some values are more probable
than others and do not surprise us).
</p>

<p>
Entropy can be a better option than accuracy gain to measure certainty.
</p>
</div>
</div>

<div id="outline-container-orgffc1bc2" class="outline-4">
<h4 id="orgffc1bc2">Information Gain</h4>
<div class="outline-text-4" id="text-orgffc1bc2">
<p>
Information Gain measures the expected reduction in entropy. It is defined as
</p>

<ul class="org-ul">
<li>Entropy of region \(R\) or parent node before the split: \(H(R)\)</li>
<li>entropy of region \(R_1\) and \(R_2\) or leaf nodes after the split: \(H(R_1)\) and \(H(R2)\)</li>
<li>Information gain is defined as
\[IG = H(R) - \frac{|R_1|H(R_1) + |R_2|H(R_2)}{|R|}\]</li>
</ul>

<p>
The feature and the corresponding value with the highest information gain will be selected for splitting.
</p>
</div>
</div>

<div id="outline-container-orgdbee5dc" class="outline-4">
<h4 id="orgdbee5dc">Gini index</h4>
<div class="outline-text-4" id="text-orgdbee5dc">
<p>
Gini Index is another widely used metric for choosing a good split in
decision trees.  The Gini index measures the "impurity" (or
non-homoneneity) at a given modes as \[GINI(R)=1-\sum_j [p(j|R)]^2\]
where \(p(j|R)\) is the class frequency of class j in region R
</p>

<ul class="org-ul">
<li>intuitively, it aims to measure the probability of misclassifying a
randomly chosen element</li>
<li>greater the value of the Gini index, the greater the changes of
having misclassificaiton</li>
<li>thus, we will look for greater gini values</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org509fa96" class="outline-3">
<h3 id="org509fa96">Overfitting</h3>
<div class="outline-text-3" id="text-org509fa96">
<ul class="org-ul">
<li>decision trees have a structure that is determined by the data</li>
<li>as a result they are flexible and can easily fit the training set,
with high risk of overfitting</li>
<li>what we can do is cut branches of the tree and replaceing it by a
leath node (<sub>pruning</sub>_)</li>
</ul>
</div>
</div>

<div id="outline-container-org10670e4" class="outline-3">
<h3 id="org10670e4">Limitations</h3>
<div class="outline-text-3" id="text-org10670e4">
<ul class="org-ul">
<li>you have exponentially less data al lower levels</li>
<li>a large tree can overfit the data</li>
<li>decision trees do not necessarly reach the global minima</li>
<li>mistakes at top-level propagate down tree</li>
</ul>

<p>
Decision trees can also be used for regression on real-valued outputs
by choosing the squared error rather than maximizing the information
gain.
</p>
</div>
</div>

<div id="outline-container-org1bc00a1" class="outline-3">
<h3 id="org1bc00a1">Comparison to KNN</h3>
<div class="outline-text-3" id="text-org1bc00a1">
<p>
Advantages and Decision Trees over KNN
</p>

<ul class="org-ul">
<li>good with descrete attributes</li>
<li>easily deals with missing vales</li>
<li>robust to scale of inputs</li>
<li>good when there are lots of attributes, but only a few are important</li>
<li>fast at test time</li>
<li>more interpretable</li>
</ul>

<p>
If some features are more important than others, you may want to
choose a decision tree.
</p>

<p>
Strengths:
</p>

<ul class="org-ul">
<li>fast and simple to implement</li>
<li>can convert to rules</li>
<li>allows for batch training</li>
</ul>

<p>
Disadvantages:
</p>

<ul class="org-ul">
<li>univariate feature split</li>
<li>requires fixed-length feature vectors</li>
<li>non-incremental (batch method)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orga60f3d7" class="outline-2">
<h2 id="orga60f3d7">Improvement to decision trees: Random Forests</h2>
<div class="outline-text-2" id="text-orga60f3d7">
<p>
<span class="underline">Bootstrapping</span> is a resampling technique that involves repeatedly
drawing samples from the dataset with replacement (the same element
can be selected multiple times)
</p>

<ul class="org-ul">
<li>this creates multiple datasets that introduce variability, reducing overfitting in models</li>
</ul>

<p>
<span class="underline">Bagging</span> (or Bootstrap Aggregation) involves training multiple
decision trees on different bootstrapped samples and averaging their
outputs (for regression) or majority voting (for classification)
</p>

<ul class="org-ul">
<li>In addition to bootstrapping, Random Forests introduce feature
randomness, this descreases the correlation between each DT and
increases its predictive accuracy on average. In other words, avoid
very strong predictive features that lead to similar split in trees</li>
</ul>

<p>
Algorithm:
</p>

<ol class="org-ol">
<li>Draw multiple bootstrapped datasets from the original data</li>
<li>Train a DT on each dataset using a random subset of \(sqrt(d)\)
features at each split</li>
<li>Aggregate predictions:
<ol class="org-ol">
<li>classification: majority vote</li>
<li>regression: average predictions</li>
</ol></li>
</ol>


<hr />

<p>
Travel: <a href="intro-to-machine-learning.html">Intro to Machine Learning</a>, <a href="../../../theindex.html">Index</a>
</p>
</div>
</div>
</div>
</body>
</html>
