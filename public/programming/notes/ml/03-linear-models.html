<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="/simple.css" />
<meta property="og:title" content="Giovanni's Diary">
<meta property="og:description" content="Diary of Giovanni's adventures.">
<meta property="og:image" content="https://giovanni-diary.netlify.app/logo.png">
<meta property="og:url" content="https://giovanni-diary.netlify.app/">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../../index.html">Giovanni's Diary</a> &gt; <a href="../../../subjects.html">Subjects</a> &gt; <a href="../../programming.html">Programming</a> &gt; <a href="../notes.html">Notes</a> &gt; <a href="intro-to-machine-learning.html">Intro to Machine Learning</a> &gt;
</p>

<div id="outline-container-orgccd3d42" class="outline-2">
<h2 id="orgccd3d42">Linear Models</h2>
<div class="outline-text-2" id="text-orgccd3d42">
<p>
We have seen the KNN algorithm for classification. Said algorithm
works everywhere as long as there is some spacial correlation in the
data, which is often the case. We will now focus on a stricter set of
problems, we will consider data that is <span class="underline">linearly separable</span>.
</p>

<p>
We say that labeled data is linearly separable if the classes can be
binary separated by a line in 2 dimensions, or using hyperplanes in
higher dimensions. In other words, a hyperplane defines a partition of
the space and the data can be classified based on in which partition
they live. This is a strong assumption to make and does not hold for
many problems, we will see in a later chapter how we can circumvent
this.
</p>

<p>
The equation of a plane in n dimensions looks like this
</p>

<p>
\[(1)\ b+\sum_{i=1}^{n}w_ix_i=0\]
</p>

<p>
Where \(w_i\) are the values of the normal an \(x_i\) are points in space
in the dimension \(i\).
</p>

<p>
We can classify a linear model by evaluating the equation \((1)\) with
an input point and checking the sign of the output: positive values
are classified as one thing, negative values as the other (remember
that we are assuming binary classification).
</p>

<p>
More formally, given:
</p>

<ul class="org-ul">
<li>an input space \(X\)</li>
<li>an unknown distribution \(D\) over \(X \times \{ -1,+1 \}\)</li>
<li>a training set D sampled from \(D\)</li>
</ul>

<p>
Compute: A function \(f\) minimizing \(\mathbb{E}_{(x, y)\sim D}[f(x) \ne  y]\) 
</p>
</div>

<div id="outline-container-org9675020" class="outline-3">
<h3 id="org9675020">Training a linear model</h3>
<div class="outline-text-3" id="text-org9675020">
<p>
Differently from KNN, linear models use <span class="underline">online learning</span>. The
learning algorithm follows the following structure:
</p>

<ul class="org-ul">
<li>the algorithm receives an unlabeled example \(x_i\)
<ul class="org-ul">
<li>For example: \(w(1, 0),\ x_i(-1,1)\)</li>
</ul></li>
<li>the algorithm predicts a classification of this example
<ul class="org-ul">
<li>Evaluate the sum equation \(b+\sum_{i=1}^n w_ix_i\): \(1*(-1)+0*1 = -1\)</li>
</ul></li>
<li>the algorithm is them told the correct answer \(y_i\) and it updates
the model only if the answer is incorrect
<ul class="org-ul">
<li>check if the sign of the equation represents the right label, if
not, add to each weight the current input times the label
(assuming the label is either \(+1\) or \(-1\))</li>
</ul></li>
</ul>

<pre class="example">
repeat until convergence (or some # of iterations):
  for each random training example (x1, x2, ..., xn, label):
    check if prediction is correct based on the current model
    if not correct, update all the weights:
      for each wi:
        wi = wi + xi*label
      b = b + label
</pre>

<p>
The algorithm will converge only if the data can be linearly
separated. The reason why \(w_i + f_i\cdot label\) improves the solution
will be obvious later when we will discuss gradient descent.
</p>
</div>

<div id="outline-container-org0acaf3b" class="outline-4">
<h4 id="org0acaf3b">Learning Rate</h4>
<div class="outline-text-4" id="text-org0acaf3b">
<p>
When the model makes an incorrect prediction, you hay have noticed
that the correction of the weights are multiple of the label
(\(f_i\cdot label\) is added to the weight). This may make convergence
difficult if we need more precise weights, so we multiply this by a
<span class="underline">learning rate</span>.
</p>

<p>
\[w_i:=w_i+\lambda x_i*label\]
</p>

<hr />

<p>
Travel: <a href="intro-to-machine-learning.html">Intro to Machine Learning</a>, <a href="../../../theindex.html">Index</a>
</p>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
