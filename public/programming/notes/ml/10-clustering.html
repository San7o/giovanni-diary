<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />

<link rel="stylesheet" href="https://san7o.github.io/micro-style.css/micro-style.css" />
<style>
  html, body {
      height: 100%;
      margin: 0;
  }

  body {
      display: grid;
      place-items: center;  /* centers vertically & horizontally */
  }
</style>
<link rel="icon" href="/giovanni-diary/favicon.ico" type="image/x-icon">
<meta property="og:title" content="Giovanni's Diary">
<meta property="og:description" content="Diary of Giovanni's adventures.">
<meta property="og:image" content="https://san7o.github.io/giovanni-diary/logo.png">
<meta property="og:url" content="https://san7o.github.io/giovanni-diary/">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../../index.html">Giovanni's Diary</a> &gt; <a href="../../../subjects.html">Subjects</a> &gt; <a href="../../programming.html">Programming</a> &gt; <a href="../notes.html">Notes</a> &gt; <a href="intro-to-machine-learning.html">Intro to Machine Learning</a> &gt;
</p>
<div id="outline-container-org9665add" class="outline-2">
<h2 id="org9665add">Clustering</h2>
<div class="outline-text-2" id="text-org9665add">
<p>
Given unlabeled data, we want to group them into clusters. The first
thing to think about is how should these clusters be represented. We
may want to model a cluster as a circumference where all the data
inside it belongs to the same cluster, or maybe an ellipse may be a
better choice for some problems. Some other data may need a completely
another structure. Ultmatively, how we represent the cluster will
characterize the algorithm that we will develop. In this chapter we
will discuss four techniques to achive this.
</p>

<p>
Some complications rise when discussing representation of data, how to
measure similarity or distance, is it a flat clustering or
hierarchical and is the number of clusters known a priori.
</p>

<p>
Let's start by categorizing <span class="underline">hard clustering</span> where each example
belongs to exactly one cluster, and <span class="underline">soft clustering</span> where an example
can belong to more than one cluster (probabilistic).
</p>
</div>
<div id="outline-container-org6ea5453" class="outline-3">
<h3 id="org6ea5453">K-means clustering</h3>
<div class="outline-text-3" id="text-org6ea5453">
<p>
First, we are assuming that we know the number of clusters. This is a
strong assumption to make and may be impossible for some problems. Our
goal is then to find an assignment of data points to clusters, as well
as a set of vectors \(\{u_k\}\), such that the sum of the squares of the
distances of each data point to its closest vector \(u_k\), is a
minimum. We can think of \(u_k\) as the center of a cluster \(C\).
</p>

<p>
\[min_{C_1, ..., C_k}\sum_{j=1}^kV(C_j)\]
</p>

<ul class="org-ul">
<li>the variation is typically given by \(V(C_j) = \sum_{i\in
  C_j} ||x_i-u_j||^2\) which is the euclidean distance squared.</li>
<li>the <span class="underline">centroid</span> is computed with \(u_j = \mathbb{E}_{c\in C}[x\in C] = \frac{1}{|C_j|}\sum_{i\in C_j} x_i\)</li>
</ul>
</div>
<div id="outline-container-org96d22d3" class="outline-4">
<h4 id="org96d22d3">The algorithm</h4>
<div class="outline-text-4" id="text-org96d22d3">
<p>
```
Initialization: Use some initialization strategy to get
some initial cluster centroids u1,&#x2026;,uk
</p>

<p>
while clusters change, do
	Assign each datapoint to the closest centroid forming
	new clusters
</p>

<p>
Cj = i in N such taht j = argmin_l(xi - ul)
</p>

<p>
	Compute cluster centroids u1,&#x2026;,uk
```
</p>

<p>
The algorithm is guaranteed to converge, but not to find the global minimum.
</p>
</div>
</div>
<div id="outline-container-org349be69" class="outline-4">
<h4 id="org349be69">Initial Centroid Selection</h4>
<div class="outline-text-4" id="text-org349be69">
<ul class="org-ul">
<li>random selection</li>
<li>points least similar to any existing center</li>
<li>try multiple starting points</li>
</ul>
</div>
</div>
<div id="outline-container-orgf8eb23f" class="outline-4">
<h4 id="orgf8eb23f">Running time</h4>
<div class="outline-text-4" id="text-orgf8eb23f">
<ul class="org-ul">
<li>assignment: \(O(kn)\) time</li>
<li>centroid computation: \(O(n)\) time</li>
</ul>

<p>
In this case, euclidian distance is not the best so we use <span class="underline">consine similarity</span>
\[sim(x, y)=\frac{x*y}{|x||y|}\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgf53cc49" class="outline-3">
<h3 id="orgf53cc49">Expectation Maximization</h3>
<div class="outline-text-3" id="text-orgf53cc49">
<p>
If we restrain our problem by assuming that the clusters are formed as
a mixture of Gaussians (elliptical data) and we assign data to a
cluster with a certain probability (soft clustering) we can use the
<span class="underline">Expectation Maximization</span> (EM) algoritm. The algorithm proceeds as
follows:
</p>

<ul class="org-ul">
<li>Start with some initial cluster centers</li>
<li>Iterate
<ul class="org-ul">
<li>Soft assign points to each cluster by calculating the probability of each point belonging to each cluster \(p(\theta _c | x)\).</li>
<li>Train: Recalculate the cluster centers by calculating the maximum likelihood cluster centers given the current soft clustering \(\theta _c\).</li>
</ul></li>
</ul>

<p>
A Gaussian (ellipse) in 1 dimension is defined as:
\[f(x)=\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma ^2}}\]
A Gaussian in \(n\) dimensions is defined as:
\[N[x;\mu , \Sigma] = \frac{1}{(2\pi)^{d/2}\sqrt{det(\Sigma)}}exp[-\frac{1}{2}(x-\mu)^T\Sigma ^{-1}(x-\mu)]\]
</p>

<p>
We learn the means of each cluster (i.e. the center) and the covariance matrix (i.e. how spread out it is in any given direction).
</p>

<p>
Intuitively, each iteration of Expectation Maximization increases the likelihood of the data and is guaranteed to converge (though to a local optimum).
</p>
</div>
</div>
<div id="outline-container-org5b3e10a" class="outline-3">
<h3 id="org5b3e10a">Spectral Clustering</h3>
<div class="outline-text-3" id="text-org5b3e10a">
<p>
In case of non-gaussian data, we can use a technique called <span class="underline">spectral
clustering</span> where we group points based on links in a graph.
</p>

<p>
We first create a fully connected graph or a k-nearest neighbor graph
(each node is only connected to its K closest
neighbors). Alternatively, we can create a graph with some notion of
similarity among nodes, for example using a gaussian kernel: \(W(i,
j)=exp[\frac{-|x_i-x_j|^2}{\sigma ^2}]\).
</p>

<p>
We then partition the graph by cutting the graph: we want to find a
partition of the graph into two parts \(A\) and \(B\) where we minimize
the sum of the weights of the set of edges that connect the two
groups, aka \(Cut(A, B)\). We observe that this problem can be
formalized as a discrete optimization problem and then relaxed in the
continuous domain and become a generalized eigenvalue problem.
</p>
</div>
</div>
<div id="outline-container-org722d5e8" class="outline-3">
<h3 id="org722d5e8">Hierarchical Clustering</h3>
<div class="outline-text-3" id="text-org722d5e8">
<p>
We are interested in producing a set of nested clusters organized as a
hierarchical tree, also called <span class="underline">dendrogram</span>.
</p>

<p>
The idea of the algorithm is to first mege very similar instances and
then incrementally build larger clusters out of smaller clusters. The
algorithm goes as follows:
</p>

<ul class="org-ul">
<li>Initially, each instance has its own cluster</li>
<li>Repeat
<ul class="org-ul">
<li>Pick the two closest clusters</li>
<li>Merge them into a new cluster</li>
<li>Stop when there is only one cluster left</li>
</ul></li>
</ul>

<hr />

<p>
Travel: <a href="intro-to-machine-learning.html">Intro to Machine Learning</a>, <a href="../../../theindex.html">Index</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
