<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="/simple.css" />
<meta property="og:title" content="Giovanni's Diary">
<meta property="og:description" content="Diary of Giovanni's adventures.">
<meta property="og:image" content="https://giovanni-diary.netlify.app/logo.png">
<meta property="og:url" content="https://giovanni-diary.netlify.app/">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../../index.html">Giovanni's Diary</a> &gt; <a href="../../../subjects.html">Subjects</a> &gt; <a href="../../programming.html">Programming</a> &gt; <a href="../notes.html">Notes</a> &gt; <a href="intro-to-machine-learning.html">Intro to Machine Learning</a> &gt;
</p>

<div id="outline-container-orge88a968" class="outline-2">
<h2 id="orge88a968">Machine Learning Basics</h2>
<div class="outline-text-2" id="text-orge88a968">
<p>
Machine Learning is the study of computer algorithms that improve
automatically through experience. How they achieve this is the
discussion of this book. In this chapter we will see a broad overview
of the types of machine learning that we will study, as well as making
some distinctions and defining some terminology. In future chapters we
will discuss the algorithms more in detail.
</p>

<p>
In machine learning we want the computer to automatically detect
patterns and predict future data from other data. To do so, we perform
a series of steps which we call a <span class="underline">pipeline</span>:
</p>

<ul class="org-ul">
<li><span class="underline">Data acquisition</span>: We want to collect the relevant data for the
problems at hand.</li>
<li><span class="underline">Preprocessing</span>: cleaning and preparing for analysis (missing values,
formatting&#x2026;). Techniques include normalization, feature scaling,
handling categorical variables.</li>
<li><span class="underline">Dimensionality reduction</span>: selection methods can be applied to
reduce the number of features while preserving the most important
information.</li>
<li><span class="underline">Model learning</span>: a model is trained on the preprocessed data.</li>
<li><span class="underline">Model testing</span>: the model is evaluated using a test set.</li>
</ul>
</div>

<div id="outline-container-orgaaffde6" class="outline-3">
<h3 id="orgaaffde6">Types of learning</h3>
<div class="outline-text-3" id="text-orgaaffde6">
<p>
Below are described the types of learning which will be discussed in
this book:
</p>
</div>

<div id="outline-container-org4a78289" class="outline-4">
<h4 id="org4a78289">Supervised Learning</h4>
<div class="outline-text-4" id="text-org4a78289">
<p>
Given labeled examples, create the model to predict the label, and
test if the predicted label is correct.
</p>

<ul class="org-ul">
<li><p>
<span class="underline">Classification</span>: there is a finite set of labels to predict.
Given a training set \(T = \{(x_1,y_1),...,(x_m,y_m)\}\) of size m,
learn a function to predict \(y\) given \(x\):
</p>

<p>
\[f: \mathbb{R}^d \rightarrow \{1, 2, ..., k\}\]
</p>

<p>
\(x\) is generally multidimensional (multiple features).
Applications include:
</p>
<ul class="org-ul">
<li>Face recognition</li>
<li>Character recognition</li>
<li>Spam detection (classical problem)</li>
<li>Medical diagnosis</li>
<li>Biometrics</li>
<li><span class="underline">Regression</span>: the label is real value:</li>
</ul></li>
</ul>
<p>
\[f: \mathbb{R}^d \rightarrow \mathbb{R}\]
</p>
<ul class="org-ul">
<li><span class="underline">Ranking</span>: label indicates an order.</li>
</ul>
</div>
</div>

<div id="outline-container-org0f5bf65" class="outline-4">
<h4 id="org0f5bf65">Unsupervised Learning</h4>
<div class="outline-text-4" id="text-org0f5bf65">
<p>
The input is given with no labels. The main problems include:
</p>

<ul class="org-ul">
<li><span class="underline">Clustering</span>: Given an input \(T = \{x_1, ..., x_m\}\), output the
hidden structure behind the \(x\)'s, which represents the
clusters. Possible applications are:
<ul class="org-ul">
<li>social network analysis</li>
<li>genomics</li>
<li>image segmentation</li>
<li>anomaly detection</li>
</ul></li>
<li><span class="underline">Dimensionality Reduction</span>: reduce the number of features under
consideration by mapping data into another low dimensional space.</li>
</ul>
<p>
\[ f: \mathbb{R}^d \rightarrow \mathbb{R}^m, m << d \]
</p>
<ul class="org-ul">
<li><span class="underline">Density estimation</span>: find a probability distribution that fits
the data.</li>
</ul>
</div>
</div>

<div id="outline-container-org373ff1b" class="outline-4">
<h4 id="org373ff1b"><b><b>Reinforcement learning</b></b></h4>
<div class="outline-text-4" id="text-org373ff1b">
<p>
The idea of reinforcement learning is that an <span class="underline">agent</span> learns from the
<span class="underline">environment</span> by interacting with it and receiving <span class="underline">rewards</span> for
performing <span class="underline">actions</span>. This framework of acting is formally known as
the <span class="underline">Markov Decision Process</span>. The agent needs to learn which actions
to take given a state to maximize the overall reward collected.
</p>
</div>
</div>

<div id="outline-container-orgbc6310b" class="outline-4">
<h4 id="orgbc6310b">Other learning variations</h4>
<div class="outline-text-4" id="text-orgbc6310b">
<ul class="org-ul">
<li><span class="underline">semi-supervised</span>: some data have have labels, and some don't.</li>
<li><span class="underline">active learning</span>: the model learns a labeled dataset, and It
interactively queries a human user to label new data points.</li>
<li><span class="underline">batch (offline) learning</span>: the model learns from the entire
dataset in one go and is updated only after processing all the data
<ul class="org-ul">
<li>Once the model is trained, it does not change.</li>
<li>Typically used when the dataset fits into memory and can be processed efficiently as a whole.</li>
</ul></li>
<li><span class="underline">online learning</span>: the model learns incrementally from each new
data point or small batches of data.
<ul class="org-ul">
<li>Allows the model to adapt to changes in the data distribution over time.</li>
<li>Suitable for scenarios where data arrives sequentially and needs to be processed in real-time or where computational resources are limited. Examples include data streams, large-scale dataset and privacy-preserving applications.</li>
</ul></li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org6d005a3" class="outline-3">
<h3 id="org6d005a3">Features</h3>
<div class="outline-text-3" id="text-org6d005a3">
<p>
Features are the questions we can ask about the examples. They are
generally represented as <span class="underline">vectors</span>.
</p>
</div>
</div>

<div id="outline-container-orgf847d6c" class="outline-3">
<h3 id="orgf847d6c">Generalization</h3>
<div class="outline-text-3" id="text-orgf847d6c">
<p>
Machine learning is about generalization of data.
</p>

<p>
Generalization in machine learning refers to the ability of a trained
model to perform well on new, unseen data that was not used during the
training process.
</p>

<p>
This can be done only if there is a correlation between inputs and
ouputs. More technically, we are going to use the <span class="underline">probabilistic
model</span> of learning.
</p>

<ul class="org-ul">
<li>there is some probability distribution over example / label pairs
called the data generating distribution</li>
<li>both the training data and the test set are generated based on the
distribution</li>
</ul>

<p>
A data <span class="underline">generating distribution</span> refers to the underlying probability
distribution that generates the observed data points in a dataset.
Understanding this distribution is crucial for building accurate and
generalizable machine learning models because It enables us to make
informed assumptions about the data and to make predictions or
decisions based on probabilistic reasoning.
This is valid for every kind of machine learning.
</p>
</div>
</div>

<div id="outline-container-org69c6af7" class="outline-3">
<h3 id="org69c6af7">Learning process</h3>
<div class="outline-text-3" id="text-org69c6af7">
<p>
The steps to take in order to learn a model are:
</p>

<ol class="org-ol">
<li>Collect (annotated) data</li>
<li>Define a family of models for the classification task</li>
<li>Define an error function to measure how well a model fits the data</li>
<li>Find the model that minimized the error, aka train or learn a model</li>
</ol>

<p>
We define the following:
</p>

<ul class="org-ul">
<li><span class="underline">task</span>: a task represents the type of prediction being made to solve
a problem on some data. \(f: x \rightarrow y\)
<ul class="org-ul">
<li>For example, in the classification case, \(f: x \rightarrow \{c_1, ..., c_k \}\).</li>
<li>Similarly is clustering, where the output is a cluster index.</li>
<li>Regression: \(f: \mathbb{R}^d \rightarrow \mathbb{R}\)</li>
<li>Dimensionality reduction: \(f: x \rightarrow y, dim(y) << dim(x)\)</li>
<li>Density estimation: \(f: x \rightarrow \Delta (x)\)</li>
</ul></li>
<li><span class="underline">data</span>: information about the problem to solve in the form of a
distribution \(p\) which is typically unknown.
<ul class="org-ul">
<li>training set: the failure of a machine learning algorithm is
often caused by a bad selection of training samples.</li>
<li>validation set</li>
<li>test set</li>
</ul></li>
<li><span class="underline">model hypotheses</span>: a model \(Ftask\) is an implementation of a function \(f\):
\[f \in Ftask\]
A set of models forms an hypothesis space:
\[Hip \subseteq Ftask\]
We use an hypothesis space to reduce the number of possible models in order to make our life easier.</li>
<li><span class="underline">learning algorithm</span>: the algorithm of your choice based on the problem</li>
<li><span class="underline">objective</span>: we want to minimize a (generalization) error function
\(E(f, p)\).
\[f* \in arg\ min\ E(f,p), f \in Ftask\]
\(Ftask\) is too big of a function space, we need an implementation
(model hypotheses) so we define a model hypothesis space \(Hip \in Ftask\) and seek a solution within that space.
\[f_{Hip}*(D) \in arg\ min_{f \in Hip_M} E(f, D)\]
With \(D=\{z_1, ..., z_n\}\) being the training data.</li>
</ul>
</div>
</div>

<div id="outline-container-org2bf9955" class="outline-3">
<h3 id="org2bf9955">Error function</h3>
<div class="outline-text-3" id="text-org2bf9955">
<p>
Let \(l(f, z)\) be a pointwise loss (a sum of point-losses). The error is computed from a function in an hypothesis space and a training set.
\[E(f, p) = \mathbb{E}_{z\sim pdata} [l(f, z)]\]
\[E(f, D) = \frac{1}{n}\sum_{i=1}^{n}l(f, z_i)\]
We want to minimize such error.
</p>
</div>
</div>

<div id="outline-container-org327cdf3" class="outline-3">
<h3 id="org327cdf3">Underfitting and Overfitting</h3>
<div class="outline-text-3" id="text-org327cdf3">
<ul class="org-ul">
<li><span class="underline">Underfitting</span>: the error is very big, the output is very "far" from the ideal solution</li>
<li><span class="underline">Overfitting</span>: there is a large gap between the generalization
(validation) and the training phase.</li>
</ul>
</div>
</div>

<div id="outline-container-org0ae0afa" class="outline-3">
<h3 id="org0ae0afa">How to improve generalization</h3>
<div class="outline-text-3" id="text-org0ae0afa">
<p>
Common techniques to improve generalization include:
</p>

<ul class="org-ul">
<li>avoid attaining the minimum on training error.</li>
<li>reduce model capacity.</li>
<li>change the objective with a regularization term:</li>
</ul>
<p>
\[E_{reg}(f, D_n) = E(f, D_n)+\lambda \Omega (f)\]
</p>

<ul class="org-ul">
<li>\(\lambda\) is the trade-off parameter</li>
<li>For example:</li>
</ul>
<p>
\[E_{reg}(f, D_n) = \frac{1}{n} \sum_{i=1}^{n} [f(x_i)-y_i]^2 + \frac{\lambda}{n} |w|^2\]
</p>

<ul class="org-ul">
<li>inject noise in the learning algorithm.</li>
<li>stop the learning algorithm before convergence.</li>
<li>increase the amount of data:</li>
</ul>
<p>
\[E(f, D_N) \rightarrow E(f, p_{data}),\ n \rightarrow \inf\]
</p>
<ul class="org-ul">
<li>augmenting the training set with transformations (rotate the image, change brightness&#x2026;).</li>
<li>combine predictions from multiple, decorrelated models (resembling).
<ul class="org-ul">
<li>train different models on different subsets of data, and we average the final solution between all of them</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org123cbf4" class="outline-3">
<h3 id="org123cbf4">Parametric vs Non-parametric Models</h3>
<div class="outline-text-3" id="text-org123cbf4">
<ul class="org-ul">
<li><span class="underline">Parametric models</span> have a finite number of parameters
<ul class="org-ul">
<li>linear regression, logistic regression, and linear support vector machines</li>
</ul></li>
<li><span class="underline">Nonparametric model</span>: the number of parameters is (potentially) infinite
<ul class="org-ul">
<li>k-nearest neighbor, decision trees, RBF kernel SVMs</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org3376d0c" class="outline-3">
<h3 id="org3376d0c">Bias</h3>
<div class="outline-text-3" id="text-org3376d0c">
<p>
The bias of a model is a measure of how strong the model assumptions are
</p>

<ul class="org-ul">
<li>low-bias classifiers make minimal assumptions about the data</li>
<li>high-bias classifiers make strong assumptions about the data</li>
</ul>

<hr />

<p>
Travel: <a href="intro-to-machine-learning.html">Intro to Machine Learning</a>, <a href="../../../theindex.html">Index</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
