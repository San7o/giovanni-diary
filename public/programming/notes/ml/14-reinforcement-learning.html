<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />

<link rel="stylesheet" href="https://san7o.github.io/micro-style.css/micro-style.css" />
<style>
  html, body {
      height: 100%;
      margin: 0;
  }

  body {
      display: grid;
      place-items: center;  /* centers vertically & horizontally */
  }
</style>
<link rel="icon" href="/giovanni-diary/favicon.ico" type="image/x-icon">
<meta property="og:title" content="Giovanni's Diary">
<meta property="og:description" content="Diary of Giovanni's adventures.">
<meta property="og:image" content="https://san7o.github.io/giovanni-diary/logo.png">
<meta property="og:url" content="https://san7o.github.io/giovanni-diary/">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../../index.html">Giovanni's Diary</a> &gt; <a href="../../../subjects.html">Subjects</a> &gt; <a href="../../programming.html">Programming</a> &gt; <a href="../notes.html">Notes</a> &gt; <a href="intro-to-machine-learning.html">Intro to Machine Learning</a> &gt;
</p>
<div id="outline-container-org5820b58" class="outline-2">
<h2 id="org5820b58">Reinforcement Learning</h2>
<div class="outline-text-2" id="text-org5820b58">
<p>
Reinforcement learning in inspired by research on psychology and
animal learning. The problems involve an agent interacting with an
environment, which provides numeric reward signals.
</p>

<p>
The model is asked to take actions with an effect on the state of the
environment in order to maximize reward.
</p>
</div>
<div id="outline-container-orgeca9611" class="outline-3">
<h3 id="orgeca9611">Markov Decision Process</h3>
<div class="outline-text-3" id="text-orgeca9611">
<p>
<span class="underline">Markov Decision Process</span> (MDP) is a framework used to help to make
decisions on a stochastic environment. Our goal is to find a policy,
which is a map that gives us all optimal actions on each state on our
environment,
</p>

<p>
In order to solve MDP we use Bellam equation which divides a problem
into simpler sub-problems easier to solve (Dynamic Programming).
</p>

<p>
The components of MDP include:
</p>

<ul class="org-ul">
<li><span class="underline">states</span> \(s\), beginning with initial state \(s_0\)</li>
<li><span class="underline">Actions</span> \(a\)</li>
<li><span class="underline">Transition model</span> \(P(s'|s, a)\)</li>
<li><span class="underline">Markov assumption</span>: the probability of going to \(s'\) from \(s\)
depends only on \(s\) and not on any other past actions or states</li>
<li><span class="underline">Reward function</span> \(r(s)\)</li>
<li><span class="underline">Policy</span> \(\pi(s)\): the action that an agent takes in any given state</li>
</ul>

<p>
Therefore, MDP is defined by:
</p>

<p>
\[(S, A, R, P, \gamma)\]
</p>

<ul class="org-ul">
<li>\(S\) set of possible states</li>
<li>\(A\) set of possible actions</li>
<li>\(R\) distribution of reward given (state, action) pair</li>
<li>\(P\) transition probability i.e. distribution over next state given
(state, action) pair</li>
<li>\(\gamma\) discount factor</li>
</ul>

<p>
In a loop, the agent selects an action \(a_t\) and receives a reward
\(r_t\) and the next state \(s_{t+1}\). A policy \(\pi(s)\) is a function
from \(S\) to \(A\) that specifies what action to take in each state. The
objective is to find policy \(\pi^*\) that maximized cumulative
discounted reward \(\sum_{t\ge 0}\gamma^t r(s_t)\) where \(\gamma\) is the
discounting factor. This controls the importance of the future rewards
versus the immediate ones: it will make the agent optimize for short
term or long term actions.
</p>
</div>
</div>
<div id="outline-container-orgd3df91a" class="outline-3">
<h3 id="orgd3df91a">Loop</h3>
<div class="outline-text-3" id="text-orgd3df91a">
<p>
In Supervised learning: given an input \(x_i\) sampled from data
distribution, we use the model with parameters \(w\) to predict output
\(y\), then calculate the loss and update \(w\) to reduce loss with
gradient descent \(w = w-\eta \nabla l(w, x_i, y_i)\). Note that the
next input does not depend on previous inputs or agent prediction and
loss is differentiable.
</p>

<p>
In Reinforcement Learning instead, given a state \(s\), we take an
action \(a\) determined by policy \(\pi(s)\). The environment selects next
state \(s'\) based on transition model \(P(s'|s, a)\) and gives a reward
\(r(s)\) while setting the new state \(s'\). Rewards are not
differentiable w.r.t. model parameters and the agent's actions affect
the environment and help to determine next observation.
</p>

<p>
There are two main approaches for Reinforcement Learning:
</p>

<ul class="org-ul">
<li>Value-based methods: the goal of the agent is to optimize the value
function \(V(s)\) where the value of each state is the total amount of
the reward an RL agent can expect to collect over the future from a
given state.</li>
<li>Policy-based approach: we define a policy which we need to optimize
directly.</li>
</ul>
</div>
</div>
<div id="outline-container-org63a33d7" class="outline-3">
<h3 id="org63a33d7">Value Based Methods</h3>
<div class="outline-text-3" id="text-org63a33d7">
</div>
<div id="outline-container-orge32dd5f" class="outline-4">
<h4 id="orge32dd5f">Value Function</h4>
<div class="outline-text-4" id="text-orge32dd5f">
<p>
The <span class="underline">value function</span> gives the total amount of rewards the agent can
expect from a particular state to all possible states from that state.
</p>

<p>
\[V^{\pi}(s)=\mathbb{E}[\sum_{t\ge 0}\gamma^t r(s_t)|s_0 = s, \pi]\]
</p>

<p>
The optimal value of a state is the value achievable by following the
best possible policy:
</p>

<p>
\[V^*(s)=max_{\pi}\mathbb{E}[\sum_{t\ge 0}\gamma^t r(s_t)|s_0 = s, \pi]\]
</p>

<p>
It is more convenient to define the value of a state-action pair,
called <span class="underline">Q-value function</span>:
</p>

<p>
\[Q^{\pi}(s, a)= \mathbb{E}[\sum_{t\ge 0}\gamma^t r(s_t)|s_0=s, a_0=a, \pi]\]
</p>

<p>
The optimal Q-value can be used to compute the optimal policy:
</p>

<p>
\[\pi^*(s)=arg\ max_aQ^*(s, a)\]
</p>
</div>
</div>
<div id="outline-container-orga1a9d4f" class="outline-4">
<h4 id="orga1a9d4f">Bellman Equation</h4>
<div class="outline-text-4" id="text-orga1a9d4f">
<p>
There is a recursive relationship between optimal values of succesive
states and actions:
</p>

<p>
\[Q^*(s, a)=r(s)+\gamma \sum_{s'}P(s'|s, a)\max_{a'}Q^*(s', a')\]
\[=\mathbb{E}_{s'\sim P(\cdot|s, a)}[r(s)+\gamma\ max_{a'}Q^*(s', a')|s, a]\]
</p>

<p>
if the optimal state-action values for the next time-step \(Q^*(s',
a')\) are known, then the optimal strategy is to take the action that
maximizes the expected value.
</p>
</div>
</div>
<div id="outline-container-org82ce455" class="outline-4">
<h4 id="org82ce455">Algorithm</h4>
<div class="outline-text-4" id="text-org82ce455">
<ul class="org-ul">
<li>Initialize the matrix \(Q\) with zeros</li>
<li>Select a random initial state</li>
<li>For each episode (set of actions that starts on the initial state
and ends on the goal state)</li>
<li>While state is not goal state
<ul class="org-ul">
<li>Select a random possible action for the current state</li>
<li>Using this possible action consider going to this next state</li>
<li>Get maximum \(Q\) value for this next state (All actions from this next state)</li>
<li>\(Q^*(s, a)=r(s, a)+\gamma\ max_a[Q^*(s', a')]\)</li>
</ul></li>
</ul>

<p>
To find the optimal policy:
</p>

<ul class="org-ul">
<li>se current state to the initial state</li>
<li>from current state, find the action with the highest \(Q\) value</li>
<li>set current state equal to the previously found state</li>
<li>repeat steps 2 and 3 until current state is the goal state.</li>
</ul>

<p>
The problem of this algorithm is that the state spaces are huge. To
help, we can approximate Q-values using a parametric function \(Q^*(s,
a)\approx Q_w(s, a)\): we train a deep neural network that approximates
\(Q^*\).
</p>

<p>
\[Q^*(s,a)=\mathbb{E}_{s'\sim P(.|s, a)}[r(s)+\gamma\ max_{a'}Q^*(s', a')|s, a]\]
</p>

<p>
The target is:
</p>

<p>
\[y_i(s, a)=\mathbb{E}_{s'\sim P(.|s, a)}[r(s)+\gamma\ max_{a'}Q_{w_{i-1}}(s', a')|s, a]\]
</p>

<p>
The loss function would change at each iteration:
</p>

<p>
\[L_i(w_i)=\mathbb{E}_{s, a\sim \rho}[(y_i(s, a)-Q_{w_i}(s, a))^2]\]
</p>

<p>
where \(\rho\) is a probability distribution over states \(s\) and actions
\(a\) that we refer to as the <span class="underline">behaviour distribution</span>.
</p>

<p>
The gradient update then is:
</p>

<p>
\[\nabla_{w_i}L(w_i)=\mathbb{E}_{s, a\sim \rho }[(y_i(s, a)-Q_{w_i}(s, a))\nabla_{w_i}Q_{w_i}(s, a)]\]
\[= \mathbb{E}_{s, a\sim \rho, s'}[(r(s)+\gamma\ max_{a'}Q_{w_{i-1}}(s', a')-Q_{w_i}(s,a))\nabla_{w_i}Q_{w_i}(s, a)]\]
</p>

<p>
Instead of having expecatations, we sample <span class="underline">experiences</span> \((s, a, s')\) using behaviour distribution and transition model.
</p>
</div>
</div>
</div>
<div id="outline-container-org5de36b1" class="outline-3">
<h3 id="org5de36b1">Policy Gradient Methods</h3>
<div class="outline-text-3" id="text-org5de36b1">
<p>
We have seen that the space of the Q-value function can be very
complicated. Instead of indirectly representing the policy using
Q-values, it can be more efficient to parametrize \(\pi\) and learn it
directly.
</p>

<p>
\[\pi_{\theta}(s, a)\approx P(a|s)\]
</p>

<p>
The idea is to use a machine learning model that will learn a good
policy from playing the game and receiving rewards. In particular, we
need to find the best parameters \(\theta\) of the policy to maximize
the expected reward:
</p>

<p>
\[maximize\ J(\theta)=\mathbb{E}[\sum_{t\ge 0}\gamma^tr_t|\pi_{\theta}]=\mathbb{E}_\tau[r(\tau)],\ \tau=(s_0, a_0, r_0, s_1, a_1, r_1, ...)\]
\[=\int_{\tau}r(\tau)p(\tau; \theta)d\tau\]
</p>

<p>
Where \(p(\tau; \theta)\) is the probability of trajectory \(\tau\) under policy with parameters \(\theta\):
</p>

<p>
\[p(\tau; \theta)=\prod_{t\ge 0}\pi_{\theta}(s_t, a_t)P(s_{t+1}|s_t, a_t)\]
</p>

<p>
We can then use gradient ascent:
</p>

<p>
\[\nabla J(\theta)=\mathbb{E}_{\tau}[r(\tau)\nabla_{\theta}\ \log\ p(\tau; \theta)]\]
\[\nabla_\theta\ log\ p(\tau; \theta)=\sum_{t\ge0}\nabla_{\theta}\log\ \pi_{\theta}(s_t, a_t)\]
Using a stochastic approximation by sampling \(n\) trajectories:
</p>

<p>
\[\nabla_{\theta}J(\theta)\approx\frac{1}{N}\sum_{1}^{N}(\sum_{t=1}^{T_i}\gamma^tr_{i, t})(\sum_{t=1}^{T_i}\nabla_{\theta} \log \pi_{\theta}(s_{i, t}, a_{i, t}))\]
</p>

<p>
Therefore the steps to perform in order to optimize are:
</p>

<ol class="org-ol">
<li>Sample \(N\) trajectories \(\tau_i\)  using current policy \(\pi_{\theta}\)</li>
<li>Estimate the policy gradient \(\nabla_{\theta}J(\theta)\)</li>
<li>Update parameters by gradient ascent \(\theta \leftarrow \theta + \eta \nabla_{\theta}J(\theta)\)</li>
</ol>

<p>
Intuitively, we want to go up the gradient to increase the total reward.
</p>

<hr />

<p>
Travel: <a href="intro-to-machine-learning.html">Intro to Machine Learning</a>, <a href="../../../theindex.html">Index</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
