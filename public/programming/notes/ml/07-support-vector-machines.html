<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />

<link rel="stylesheet" href="https://san7o.github.io/micro-style.css/micro-style.css" />
<style>
  html, body {
      height: 100%;
      margin: 0;
  }

  body {
      display: grid;
      place-items: center;  /* centers vertically & horizontally */
  }
</style>
<link rel="icon" href="/giovanni-diary/favicon.ico" type="image/x-icon">
<meta property="og:title" content="Giovanni's Diary">
<meta property="og:description" content="Diary of Giovanni's adventures.">
<meta property="og:image" content="https://san7o.github.io/giovanni-diary/logo.png">
<meta property="og:url" content="https://san7o.github.io/giovanni-diary/">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../../index.html">Giovanni's Diary</a> &gt; <a href="../../../subjects.html">Subjects</a> &gt; <a href="../../programming.html">Programming</a> &gt; <a href="../notes.html">Notes</a> &gt; <a href="intro-to-machine-learning.html">Intro to Machine Learning</a> &gt;
</p>
<div id="outline-container-orgeb4dd7f" class="outline-2">
<h2 id="orgeb4dd7f">Support Vector Machines</h2>
<div class="outline-text-2" id="text-orgeb4dd7f">
<p>
Linear classifiers assume the data can be linearly separable. If this
does not hold, the chosen hyperplane will keep adjusting at each
iteration without ever converging. Let's now discuss a particular type
of classifier: support vector machines (SVM).
</p>
</div>
<div id="outline-container-org4d117e5" class="outline-3">
<h3 id="org4d117e5">Large margin classifiers</h3>
<div class="outline-text-3" id="text-org4d117e5">
<p>
We define the <span class="underline">margin</span> of a classifier as the distance from the
classification hyperplane to the closest points of either class. We
call <span class="underline">large margin classifiers</span> the classifiers that attempt to
maximize this measure.
</p>

<p>
We call the sets of "closest points" from the hyperplane with <span class="underline">support
vectors</span>. Note that for n dimensions, there will be at least \(n+1\)
support vectors; this is because a plane is identified by \(n+1\) points in \(n\) dimensions.
</p>

<p>
Therefore, large margin classifiers only use support vectors and
ignore all other training data.
</p>

<p>
Let's formalize the large margin classifier with algebra.
</p>

<p>
We uniquely identify the decision hyperplane by a normal vector w and a
scalar b which determines which of the planes perpendicular to w to
choose:
</p>

<p>
\[\vec{w}\vec{x} + b = 0\]
</p>

<p>
We are interested in maximizing the distance from the hyperplane to
the support vectors. Let us first find the distance from the
hyperplane to a point p. We know that the distance vector must be
perpendicular to the hyperplane, therefore it is parallel to the
normal vector w, which when normalized is \(\frac{\vec{w}}{||\vec{w}||}\).
</p>

<p>
Therefore, the distance between the hyperplane and \(p_1\), referred to as
\(r\), is the difference between \(p_1\) and the closest point in
the hyperplane \(p_2\), which is the intersection between the line parallel
to the normal and point \(p1\):
\[(1)\ r\frac{\vec{w}}{||\vec{w}||} = \vec{p_1} - \vec{p_2}\]
</p>

<p>
Moreover, since \(p2\) lies on the hyperplane, it is ture that:
\[(2)\ \vec{w}\vec{p_2} + b = 0\]
We find \(p2\) from \((1)\), \(p_2 = p_1 - r\frac{\vec{w}}{||w||}\). We substitute
\(p_2\) in \((2)\) and solve for r, the distance, and we get 
\[r = \frac{(\vec{w}\vec{p_1} + b) ||\vec{w}||}{\vec{w}\vec{w}}\]
Since \(\vec{w}\vec{w}=||w||^2\), this can be easily demonstrated because
the modulo function uses the generalized pythagorean theorem, we get:
\[r = \frac{(\vec{w}\vec{p_1} + b)}{||\vec{w}||}\]
</p>

<p>
We just demonstrated the formula for finding the distance between a
point and a plane.  We now define the <span class="underline">classification function</span> as
\(f(x)= sign(\vec{w}\vec{x} + b)\), it will be \(+1\) or \(-1\) based on the
sign of the equation. Intuitively, points on one side of the plane
should have a positive output and points on the other side should have
a negative one.
</p>

<p>
We want to maximize this distance, but the value is positive or
negative depending on where the point is, hence we multiply the
hyperplane equation times the expected classification in roder to make
the distance always positive (labels are either \(-1\) or \(1\)). Indeed,
If we assign the label \(y_i=-1\) for the negative distance points, we
will always get a positive output (negative times negative is
negative, and positive times positive is positive). We define the
<span class="underline">functional margin</span> as double such quantity: this represents the
distance between the two closest points from the plane with different
labels: \[margin = 2\frac{y_i(\vec{w}\vec{x_i}+b)}{||\vec{w}||}\] For
convenience, we require that the distance between the hyperplane and
the support vectors is 1, therefore \(y_i(\vec{w}\vec{x_i}+b) \ge 2
\forall i\) because negative distances will be multiplied by \(y_i\)
which is also negative, resulting in a positive value all the time,
and a greater than \(1\) because of the requirement.
</p>

<p>
For the support vectors, the following holds:
\[margin = \frac{1}{||\vec{w}||}\]
</p>
</div>
</div>
<div id="outline-container-orga7cf892" class="outline-3">
<h3 id="orga7cf892">Maximizing the margin</h3>
<div class="outline-text-3" id="text-orga7cf892">
<p>
We want to maximize this margin, therefore we need to minimize \(\frac{1}{2}||\vec{w}||\)
with the constraint that \(y_i(\vec{w}\vec{x_i}+b) \ge 1\ \forall i\). We
may as well minimize \(\frac{1}{2}||\vec{w}||^2\) which is an example of a <span class="underline">quadratic
optimization problem</span>
\[ min_{w, b}\ \frac{1}{2} ||w||^2,\ y_i(\vec{w}\vec{x_i}) \ge 1 \forall i \]
</p>
</div>
</div>
<div id="outline-container-org67ca461" class="outline-3">
<h3 id="org67ca461">Soft margin classification</h3>
<div class="outline-text-3" id="text-org67ca461">
<p>
We may have data that is not perfect, for example we may have some
values that lay on one side of the hyperplane but they are classified
as the other, so \(y_i(\vec{w}\vec{x_i}+b) \ge 1 \forall i\) does not
hold since you may have \(\vec{w}\vec{x_i}+b \le 1\) and \(y_i \ge 1\) or
the opposite.
</p>

<p>
To address this cases, we introduce <span class="underline">slack variabes</span> which are
scalar values assigned for each \(x_i\), and the <span class="underline">regularizaion
parameter</span> \(C > 0\):
\[ min_{w, b} ||\vec{w}||^2 + C \sum_{i} \zeta_i \]
subject to:
\[y_i(\vec{w}\vec{x_i} + b) \ge 1 - \zeta_i \forall i, \zeta_i \ge 0\]
</p>

<p>
with this correction, the values are "allowed to have mistakes", the
margin is reduced by \(\zeta\) if \(\zeta < 1\) or are moved to the
other side of the hyperplane if \(\zeta > 1\).
\(C\) determines how strong are those corrections, that is the "trade off"
between the slack variable penalty and the margin.
</p>

<ul class="org-ul">
<li>small C allows constrains to be easily ignored in order to have a
larger margin</li>
<li>large C makes constraints hard to ignore in order to get a narrow margin
<ul class="org-ul">
<li>if C goes to infinity, we have hard margins.</li>
</ul></li>
</ul>

<p>
We are now interested into calculating these slack variables. The
first observation we make is that if the constraint is already
satisfied, meaning that if \(y_i(\vec{w}\vec{x_i} + b) \ge 1\) and the
value has not been misclassified, then we don't need any correction and
\(\zeta = 0\). Otherwise, we want to correct the value by moving it on
the other side, plus the distance to the margin 1, so \(1 - y_i(\vec{w}\vec{x_i}+b)\). We are subtracting because if the value is
misclassified, then it's distance from the hyperplane times It's label
is going to be a negative value (one of them nedds to be positive and
the other negative in order to have a misclassification).
</p>

<p>
Therefore:
\[\zeta = 0\ if\ y_i(\vec{w}\vec{x_i}+b) \ge 1\]
\[\zeta = 1 - y_i(\vec{w}\vec{x_i}+b)\ otherwise\]
</p>

<p>
which is the same as the following, using a notation introduced
in previous lessons:
\[\zeta = max(0, 1-y_i(\vec{w}\vec{x_i}+b)) = max(0, 1-yy')\]
</p>

<p>
If you recall from the lesson of Gradiente Descent, this is the hinge
loss function.
</p>

<p>
With this result, the objective is now to minimize the following:
\[min_{w, b} ||\vec{w}||^2 + C \sum_i max(0, 1 - y_i(\vec{w}\vec{x_i} +b))\]
</p>
</div>
</div>
<div id="outline-container-org38183d3" class="outline-3">
<h3 id="org38183d3">Cases of optimization problems</h3>
<div class="outline-text-3" id="text-org38183d3">
<p>
For future analisys, it is useful to discuss what are the main classes
of optimization problems:
</p>

<ul class="org-ul">
<li>_linear programming) (LP): linear problem, linear constraints.</li>
</ul>

<p>
\[min_{x} c^Tx\ s.t.\ Ax = b, x \ge 0\]
</p>

<ul class="org-ul">
<li><span class="underline">quadratic programming</span> (QP): quadratic objective and linear
constraints, it is convex if the matrix \(Q\) is positive
semidefinite, that is the real number \(x^TQx\) is positive or zero for
every nonzero real column vector \(x\), where \(x^T\) is the row vector
transpose of \(x\).</li>
</ul>

<p>
\[min_{x} c^Tx + \frac{1}{2}x^TQx\ s.t\ Ax = b, Cx \ge d\]
</p>

<ul class="org-ul">
<li><span class="underline">nonlinear programming</span> (NLP): in general non-convex.</li>
</ul>
</div>
</div>
<div id="outline-container-org41de37f" class="outline-3">
<h3 id="org41de37f">Solving quadratic problems - Lagrange multipliers</h3>
<div class="outline-text-3" id="text-org41de37f">
<p>
Quadratic optimization problems such as the one discussed above are a well-known class of mathematical programming models with several algorithms. We will now introduce a method so solve such problems using the Lagrange multiplier, that is a strategy for finding the local maxima and minima of a function subject to equation constraints.
</p>

<p>
Given a function to optimize \(f(x)\), a constraint \(g(x)\) and an
optimal solution \(x_*\) of the function that respects the contraints, there
exists a <span class="underline">lagrangian multiplier</span> \(\lambda\) such that:
\[\frac{df(x_*)}{dx_*} = \lambda \frac{dg(x_*)}{dx_*},\ g(x) = 0\]
Or equivalently:
 \[\frac{df(x_*)}{dx_*} - \lambda \frac{dg(x_*)}{dx_*} = 0,\ g(x) = 0\]
 We call this the lagrangian function or <span class="underline">Lagrangian</span>:
 \[L(x) = f(x) - \lambda g(x)\]
</p>

<p>
Let's now apply this knowledge in our problem. Let \(f(x)=||\vec{w}||^2\)
and \(g(x, b, w)=y_i(\vec{w}\vec{x_i}+b)-1\), using \(a\) as the lagrangian
multiplier:
\[(a) L(x, \vec{w}, b, \vec{a}) = \frac{1}{2}||\vec{w}||^2 - \sum_i a_i (y_i(\vec{w}^T\vec{x_i} + b) - 1)\]
This is an example of Lagrangian dual problem, where we need to
maximize the Lagrangian multipliers to minimize \(w\) and \(b\). We now
derivate with respect to \(w\) and \(b\) and set them equal to \(0\):
\[(b)\ \vec{w} - \sum_i a_i y_i x_i = 0\]
\[(c)\ \sum_i a_i y_i = 0\]
</p>

<p>
From \((b)\) we get \(\vec{w} = \sum_i a_i y_i x_i\). We now
substitute the new \((b)\) in \((a)\), observing that \(||w||^2 = w^Tw\):
\[L(x, \vec{a}, b) = \frac{1}{2}\sum_i \sum_j a_i a_j y_i y_j x_i x_j - (\sum_i \sum_j a_i a_j y_i y_j x_i x_j - b\sum_i a_i j_i - \sum_i a_i) \]
\[ = -\frac{1}{2}\sum_i \sum_i \sum_j a_i a_j y_i y_j x_i x_j - (-b\sum_i a_i y_i - \sum_i a_i) \]
</p>

<p>
The second term is \(0\) because of \((c)\), so it can be eliminated, finally
we have:
\[ L(x, \vec{a}) = \sum_i a_i -\frac{1}{2}\sum_i \sum_j \sum_j a_i a_j y_i y_j x_i x_j \]
such that \(\sum_i a_i y_i = 0, 0 \le a_i \le C\ \forall i\).
</p>

<p>
This is the final equation that we need to maximize over \(a_i\) to
minimize w and b. To recap, we turned the original optimization
problem \(min_{w, b} ||\vec{w}||^2\) to a problem depending only on
lagrangian multipliers, which is faster to compute. We let the
computer solve this and get the \(a_i\) values, after that we can find \(w\)
using \((b)\) and b from \(y_k = wx_k + b\) for any \(k\) and using again \(w\)
from \((b)\).
</p>

<p>
Finally, to make predictions, we use the perceptron formula with \((b)\):
\[(d)\ f(x) = \sum_i a_iy_i x_i x + b\]
</p>

<ul class="org-ul">
<li>each non-zero \(a_i\) indicates that the corresponding \(x_i\) is a support
vector.</li>
</ul>
</div>
</div>
<div id="outline-container-org7bcdad8" class="outline-3">
<h3 id="org7bcdad8">Non linear SVM - Kernel Trick</h3>
<div class="outline-text-3" id="text-org7bcdad8">
<p>
What if the data is not linearly separable? In such situation we can
map data to a higher-dimensional space where the training set is
separable.
\[\Phi: X \rightarrow H\]
\[h=\phi(x)\]
</p>

<p>
We notice that the linear classifier (d) relies on the product between
\(x_i\) and \(x\). We can abstract this product to happen in a higher dimension
using a function called Kernel which computes the dot product over some
higher-dimensional feature mapping function \(\phi(x)\):
\[K(x_i, x_j) = \phi(x_i)\cdot \phi(x_j)\]
Therefore \((d)\) becomes:
\[ f(x) = \sum_i a_iy_i K(x_i, x) + b \]
</p>

<p>
Mercer's Theorem: every positive semidefinite symmetric function is a
kernel
</p>

<p>
There are multiple types of kernels, such as
</p>

<ul class="org-ul">
<li>linear: \(K(x_i, x_j) = x_i^T x_j\)</li>
<li>polinomial of power p: \(K(x_i, x_j) = (1+x_i^T x_j)^p\)</li>
<li>gaussian: \(K(x_i, x_j) = e^{\frac{|x_i-x_j|^2}{2\sigma ^2}}\)</li>
</ul>

<p>
To recap, kernels are generalization of dot products to arbitrary domains.
</p>



<hr />

<p>
Travel: <a href="intro-to-machine-learning.html">Intro to Machine Learning</a>, <a href="../../../theindex.html">Index</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
