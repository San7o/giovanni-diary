<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />

<link rel="stylesheet" href="https://san7o.github.io/micro-style.css/micro-style.css" />
<style>
  html, body {
      height: 100%;
      margin: 0;
  }

  body {
      display: grid;
      place-items: center;  /* centers vertically & horizontally */
  }
</style>
<link rel="icon" href="/giovanni-diary/favicon.ico" type="image/x-icon">
<meta property="og:title" content="Giovanni's Diary">
<meta property="og:description" content="Diary of Giovanni's adventures.">
<meta property="og:image" content="https://san7o.github.io/giovanni-diary/logo.png">
<meta property="og:url" content="https://san7o.github.io/giovanni-diary/">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../../index.html">Giovanni's Diary</a> &gt; <a href="../../../subjects.html">Subjects</a> &gt; <a href="../../programming.html">Programming</a> &gt; <a href="../notes.html">Notes</a> &gt; <a href="intro-to-machine-learning.html">Intro to Machine Learning</a> &gt;
</p>
<div id="outline-container-orgab06ede" class="outline-2">
<h2 id="orgab06ede">Regularization</h2>
<div class="outline-text-2" id="text-orgab06ede">
<p>
A recap on model based machine learning:
</p>

<ol class="org-ol">
<li>Pick a model</li>
</ol>
<p>
\[0=b+\sum_{j=1}^mw_if_i\]
</p>
<ol class="org-ol">
<li>Pick a criteria to optimize</li>
</ol>
<p>
\[\sum_{i=1}^n\mathbb{1}[y_i(wx_i+b)\le 0]\]
</p>
<ol class="org-ol">
<li>Develop a learning algorithm</li>
</ol>
<p>
\[argmin_{w, b}\sum_{i=1}^n\mathbb{1}[y_i(wx_i+b)\le 0]\]
</p>
<ul class="org-ul">
<li>repeat: pick a dimension and move a small amount towards the opposite of the derivative</li>
</ul>
</div>
<div id="outline-container-org9ddf319" class="outline-3">
<h3 id="org9ddf319">Regularizer</h3>
<div class="outline-text-3" id="text-org9ddf319">
<p>
A <span class="underline">regularizer</span> is an additional criterion to the loss function to make sure that we do not <span class="underline">overfit</span>
\[argmin_{w, b}\sum_{i=1}^nloss(yy')+\lambda regularizer(w, b)\]
</p>

<ul class="org-ul">
<li>we want to bias the model so that it prefers certain types of weights over others</li>
<li>note that is I sum two convex functions, the result is also convex. We want a convex regularizer. If the function is convex, there is usually an easy solution.</li>
</ul>


<p>
Generally, we do not want huge weights: if weights are large, a small change in a feature can result in a large change in the prediction
</p>

<p>
So, how do we encourage small weights or penalize large weights?
</p>
</div>
</div>
<div id="outline-container-org91b5708" class="outline-3">
<h3 id="org91b5708">Common regularizers</h3>
<div class="outline-text-3" id="text-org91b5708">
<p>
Sum of the weights
\[r(w, b)=\sum |w_j|\]
Sum of the squared weights
\[r(w, b)=\sqrt{\sum |w_j|^2}\]
</p>

<ul class="org-ul">
<li>this penalizes large values more compared to sum of weights</li>
</ul>

<p>
In general, we can call this formula <span class="underline">p-norm</span> or \(L_p (L_1, L_2, ....)\):
\[r(w, b)=\sqrt[p]{\sum |w_j|^p}=||w||^p\]
</p>
</div>
</div>
<div id="outline-container-orge312a75" class="outline-3">
<h3 id="orge312a75">Using Gradient Descent</h3>
<div class="outline-text-3" id="text-orge312a75">
<p>
Example using 2-norm:
\[argmin_{w, b}\sum_{i=1}^n exp(-y_i(wx_i+b))+\frac{\lambda}{2}||w||^2\]
\[\frac{d}{dw_j}equation = -\sum_{i=1}^n y_ix_{ij}exp(-y_i(wx_i+b))+\lambda w_i\]
We can multiply all of this by a constant to control the learning rate.
</p>

<p>
Note that gradient descent is not the only minimization method.
</p>
</div>
</div>
<div id="outline-container-org75e0b4e" class="outline-3">
<h3 id="org75e0b4e">Logistic Regression</h3>
<div class="outline-text-3" id="text-org75e0b4e">
<p>
Log loss or binary cross-entropy loss
\[L(w) = -\sum_{i=1}^n [y_i\log(\hat{y_i})+(1-y_i)\log(1-\hat{y_i})]\]
where
\[\hat{y_i} = \sigma (x_i^Tw)\]
and \[\sigma (z)=\frac{1}{1+e^{-z}}\]
</p>

<hr />

<p>
Travel: <a href="intro-to-machine-learning.html">Intro to Machine Learning</a>, <a href="../../../theindex.html">Index</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
