<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="/simple.css" />
<meta property="og:title" content="Giovanni's Diary">
<meta property="og:description" content="Diary of Giovanni's adventures.">
<meta property="og:image" content="https://giovanni-diary.netlify.app/logo.png">
<meta property="og:url" content="https://giovanni-diary.netlify.app/">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../../index.html">Giovanni's Diary</a> &gt; <a href="../../../subjects.html">Subjects</a> &gt; <a href="../../programming.html">Programming</a> &gt; <a href="../notes.html">Notes</a> &gt; <a href="intro-to-machine-learning.html">Intro to Machine Learning</a> &gt;
</p>

<div id="outline-container-org343173a" class="outline-2">
<h2 id="org343173a">Regularization</h2>
<div class="outline-text-2" id="text-org343173a">
<p>
A recap on model based machine learning:
</p>

<ol class="org-ol">
<li>Pick a model</li>
</ol>
<p>
\[0=b+\sum_{j=1}^mw_if_i\]
</p>
<ol class="org-ol">
<li>Pick a criteria to optimize</li>
</ol>
<p>
\[\sum_{i=1}^n\mathbb{1}[y_i(wx_i+b)\le 0]\]
</p>
<ol class="org-ol">
<li>Develop a learning algorithm</li>
</ol>
<p>
\[argmin_{w, b}\sum_{i=1}^n\mathbb{1}[y_i(wx_i+b)\le 0]\]
</p>
<ul class="org-ul">
<li>repeat: pick a dimension and move a small amount towards the opposite of the derivative</li>
</ul>
</div>

<div id="outline-container-org4e811ee" class="outline-3">
<h3 id="org4e811ee">Regularizer</h3>
<div class="outline-text-3" id="text-org4e811ee">
<p>
A <span class="underline">regularizer</span> is an additional criterion to the loss function to make sure that we do not <span class="underline">overfit</span>
\[argmin_{w, b}\sum_{i=1}^nloss(yy')+\lambda regularizer(w, b)\]
</p>

<ul class="org-ul">
<li>we want to bias the model so that it prefers certain types of weights over others</li>
<li>note that is I sum two convex functions, the result is also convex. We want a convex regularizer. If the function is convex, there is usually an easy solution.</li>
</ul>


<p>
Generally, we do not want huge weights: if weights are large, a small change in a feature can result in a large change in the prediction
</p>

<p>
So, how do we encourage small weights or penalize large weights?
</p>
</div>
</div>

<div id="outline-container-org5b9eb85" class="outline-3">
<h3 id="org5b9eb85">Common regularizers</h3>
<div class="outline-text-3" id="text-org5b9eb85">
<p>
Sum of the weights
\[r(w, b)=\sum |w_j|\]
Sum of the squared weights
\[r(w, b)=\sqrt{\sum |w_j|^2}\]
</p>

<ul class="org-ul">
<li>this penalizes large values more compared to sum of weights</li>
</ul>

<p>
In general, we can call this formula <span class="underline">p-norm</span> or \(L_p (L_1, L_2, ....)\):
\[r(w, b)=\sqrt[p]{\sum |w_j|^p}=||w||^p\]
</p>
</div>
</div>

<div id="outline-container-org14ea579" class="outline-3">
<h3 id="org14ea579">Using Gradient Descent</h3>
<div class="outline-text-3" id="text-org14ea579">
<p>
Example using 2-norm:
\[argmin_{w, b}\sum_{i=1}^n exp(-y_i(wx_i+b))+\frac{\lambda}{2}||w||^2\]
\[\frac{d}{dw_j}equation = -\sum_{i=1}^n y_ix_{ij}exp(-y_i(wx_i+b))+\lambda w_i\]
We can multiply all of this by a constant to control the learning rate.
</p>

<p>
Note that gradient descent is not the only minimization method.
</p>
</div>
</div>

<div id="outline-container-org56dfd9a" class="outline-3">
<h3 id="org56dfd9a">Logistic Regression</h3>
<div class="outline-text-3" id="text-org56dfd9a">
<p>
Log loss or binary cross-entropy loss
\[L(w) = -\sum_{i=1}^n [y_i\log(\hat{y_i})+(1-y_i)\log(1-\hat{y_i})]\]
where
\[\hat{y_i} = \sigma (x_i^Tw)\]
and \[\sigma (z)=\frac{1}{1+e^{-z}}\]
</p>

<hr />

<p>
Travel: <a href="intro-to-machine-learning.html">Intro to Machine Learning</a>, <a href="../../../theindex.html">Index</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
